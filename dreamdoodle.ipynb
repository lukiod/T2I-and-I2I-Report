{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukiod/T2I-and-I2I-Report/blob/main/dreamdoodle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKb0IHASOWJY",
        "outputId": "535d794f-bb0f-46a5-d837-d48086d9e1da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DreamDoodleWebApp\n"
          ]
        }
      ],
      "source": [
        "# Create directories\n",
        "!mkdir -p /content/DreamDoodleWebApp/static/uploads\n",
        "!mkdir -p /content/DreamDoodleWebApp/static/results\n",
        "!mkdir -p /content/DreamDoodleWebApp/templates\n",
        "\n",
        "# Navigate into the main project directory\n",
        "%cd /content/DreamDoodleWebApp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch matching Colab's CUDA version (usually handled automatically, but specific version helps)\n",
        "# Check !nvidia-smi for CUDA version if needed, but 2.3.0 should work with recent Colab GPUs\n",
        "!python3 -m pip install -q -U torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
        "\n",
        "# Install diffusers and related HF libraries\n",
        "!python3 -m pip install -q -U diffusers transformers accelerate Pillow matplotlib pandas numpy\n",
        "\n",
        "# Install OneDiff/Nexfort stack\n",
        "!python3 -m pip install -q -U nexfort torchao # <-- REMOVED ==0.1\n",
        "!python3 -m pip install -q --pre onediff onediffx\n",
        "\n",
        "# Install OneFlow (Optional, only if using --compiler oneflow)\n",
        "# !python3 -m pip install -q oneflow\n",
        "# Or specific wheel if needed:\n",
        "# !python3 -m pip install -q -U --pre oneflow -f https://github.com/siliconflow/oneflow_releases/releases/expanded_assets/community_cu122"
      ],
      "metadata": {
        "id": "xdlTTD6-PuE1",
        "outputId": "25ae209a-32e3-4120-d6bf-9e5a28625cec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for onefx (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generate.py\n",
        "\n",
        "# ==================================================================\n",
        "# PASTE THE ENTIRE CONTENT of your working generate.py script here\n",
        "# ==================================================================\n",
        "# Example start (replace with your full code):\n",
        "# Default Settings (Mostly T2I oriented, override via command line)\n",
        "# Default Settings (Mostly T2I oriented, override via command line)\n",
        "DEFAULT_MODEL = \"SG161222/RealVisXL_V4.0\" # Default to a T2I XL model\n",
        "DEFAULT_VARIANT = None\n",
        "DEFAULT_CUSTOM_PIPELINE = None\n",
        "DEFAULT_SCHEDULER = \"EulerAncestralDiscreteScheduler\"\n",
        "DEFAULT_LORA = None\n",
        "DEFAULT_CONTROLNET = None\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_PROMPT = \"best quality, realistic, unreal engine, 4K, a cat sitting on human lap\"\n",
        "DEFAULT_NEGATIVE_PROMPT = \"\"\n",
        "DEFAULT_SEED = 333\n",
        "DEFAULT_WARMUPS = 1\n",
        "DEFAULT_BATCH = 1\n",
        "DEFAULT_HEIGHT = None # Auto-detect from model if None\n",
        "DEFAULT_WIDTH = None  # Auto-detect from model if None\n",
        "DEFAULT_INPUT_IMAGE = None # If provided, triggers I2I mode\n",
        "DEFAULT_CONTROL_IMAGE = None\n",
        "DEFAULT_OUTPUT_IMAGE = \"generated_image.png\" # Provide a default output name\n",
        "DEFAULT_EXTRA_CALL_KWARGS = None # e.g., '{\"strength\": 0.75, \"guidance_scale\": 7.5}'\n",
        "DEFAULT_CACHE_INTERVAL = 3\n",
        "DEFAULT_CACHE_LAYER_ID = 0\n",
        "DEFAULT_CACHE_BLOCK_ID = 0\n",
        "DEFAULT_COMPILER = \"nexfort\"\n",
        "DEFAULT_COMPILER_CONFIG = None\n",
        "DEFAULT_QUANTIZE_CONFIG = None\n",
        "DEFAULT_TASK = \"auto\" # Can be 'auto', 'text2image', 'image2image', 'instructpix2pix'\n",
        "\n",
        "import os\n",
        "import importlib\n",
        "import inspect\n",
        "import argparse\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "from diffusers.utils import load_image, is_accelerate_available, is_accelerate_version\n",
        "\n",
        "# Required for onediffx/nexfort\n",
        "from onediffx import compile_pipe, quantize_pipe\n",
        "\n",
        "# --- Argument Parsing ---\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Generate images using diffusion models (T2I/I2I).\")\n",
        "    parser.add_argument(\"--model\", type=str, default=DEFAULT_MODEL, help=\"Hugging Face model ID or local path.\")\n",
        "    parser.add_argument(\"--variant\", type=str, default=DEFAULT_VARIANT, help=\"Model variant (e.g., 'fp16').\")\n",
        "    parser.add_argument(\"--custom-pipeline\", type=str, default=DEFAULT_CUSTOM_PIPELINE, help=\"Custom pipeline class path.\")\n",
        "    parser.add_argument(\"--scheduler\", type=str, default=DEFAULT_SCHEDULER, help=\"Scheduler name (e.g., 'EulerAncestralDiscreteScheduler', 'DPMSolverMultistepScheduler', 'none').\")\n",
        "    parser.add_argument(\"--lora\", type=str, default=DEFAULT_LORA, help=\"Path or Hub ID for LoRA weights.\")\n",
        "    parser.add_argument(\"--controlnet\", type=str, default=DEFAULT_CONTROLNET, help=\"Path or Hub ID for ControlNet model.\")\n",
        "    parser.add_argument(\"--steps\", type=int, default=DEFAULT_STEPS, help=\"Number of inference steps.\")\n",
        "    parser.add_argument(\"--prompt\", type=str, default=DEFAULT_PROMPT, help=\"Text prompt for generation/modification.\")\n",
        "    parser.add_argument(\"--negative-prompt\", type=str, default=DEFAULT_NEGATIVE_PROMPT, help=\"Negative prompt.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=DEFAULT_SEED, help=\"Random seed for generation. Set to None for random.\")\n",
        "    parser.add_argument(\"--warmups\", type=int, default=DEFAULT_WARMUPS, help=\"Number of warmup runs before timing.\")\n",
        "    parser.add_argument(\"--batch\", type=int, default=DEFAULT_BATCH, help=\"Number of images per prompt (batch size).\")\n",
        "    parser.add_argument(\"--height\", type=int, default=DEFAULT_HEIGHT, help=\"Image height in pixels. Defaults to model's optimal size.\")\n",
        "    parser.add_argument(\"--width\", type=int, default=DEFAULT_WIDTH, help=\"Image width in pixels. Defaults to model's optimal size.\")\n",
        "    parser.add_argument(\"--cache_interval\", type=int, default=DEFAULT_CACHE_INTERVAL, help=\"DeepCache cache interval.\")\n",
        "    parser.add_argument(\"--cache_layer_id\", type=int, default=DEFAULT_CACHE_LAYER_ID, help=\"DeepCache cache layer ID.\")\n",
        "    parser.add_argument(\"--cache_block_id\", type=int, default=DEFAULT_CACHE_BLOCK_ID, help=\"DeepCache cache block ID.\")\n",
        "    parser.add_argument(\"--extra-call-kwargs\", type=str, default=DEFAULT_EXTRA_CALL_KWARGS, help=\"JSON string of extra kwargs for the pipeline call (e.g., '{\\\"strength\\\": 0.8, \\\"guidance_scale\\\": 7.0}').\")\n",
        "    parser.add_argument(\"--input-image\", type=str, default=DEFAULT_INPUT_IMAGE, help=\"Path or URL to the input image. If provided, enables Image-to-Image mode.\")\n",
        "    parser.add_argument(\"--control-image\", type=str, default=DEFAULT_CONTROL_IMAGE, help=\"Path or URL to the ControlNet conditioning image.\")\n",
        "    parser.add_argument(\"--output-image\", type=str, default=DEFAULT_OUTPUT_IMAGE, help=\"Path to save the generated image.\")\n",
        "    parser.add_argument(\"--throughput\", action=\"store_true\", help=\"Run throughput analysis.\")\n",
        "    parser.add_argument(\"--deepcache\", action=\"store_true\", help=\"Use DeepCache optimization (if supported by the model/pipeline).\")\n",
        "    parser.add_argument(\n",
        "        \"--task\",\n",
        "        type=str,\n",
        "        default=DEFAULT_TASK,\n",
        "        choices=[\"auto\", \"text2image\", \"image2image\", \"instructpix2pix\"],\n",
        "        help=\"Specify the task type. 'auto' detects based on --input-image. Use 'instructpix2pix' for that specific pipeline.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--compiler\",\n",
        "        type=str,\n",
        "        default=DEFAULT_COMPILER,\n",
        "        choices=[\"none\", \"oneflow\", \"nexfort\", \"compile\", \"compile-max-autotune\"],\n",
        "        help=\"Compiler backend to use for optimization.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--compiler-config\",\n",
        "        type=str,\n",
        "        default=DEFAULT_COMPILER_CONFIG,\n",
        "        help=\"JSON string for compiler configuration options.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--run_multiple_resolutions\",\n",
        "        action=\"store_true\", # Simplified to a flag\n",
        "        help=\"Run tests with multiple common resolutions after the main generation.\"\n",
        "    )\n",
        "    parser.add_argument(\"--quantize\", action=\"store_true\", help=\"Enable quantization (currently requires --compiler nexfort).\")\n",
        "    parser.add_argument(\n",
        "        \"--quantize-config\",\n",
        "        type=str,\n",
        "        default=DEFAULT_QUANTIZE_CONFIG,\n",
        "         help=\"JSON string for quantization configuration.\"\n",
        "    )\n",
        "    parser.add_argument(\"--quant-submodules-config-path\", type=str, default=None, help=\"Path to quantization submodules config file (for advanced nexfort quantization).\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "# --- Utility Functions ---\n",
        "\n",
        "def load_pipe(\n",
        "    pipeline_cls,\n",
        "    model_name,\n",
        "    variant=None,\n",
        "    dtype=torch.float16,\n",
        "    device=\"cuda\",\n",
        "    custom_pipeline=None,\n",
        "    scheduler=None,\n",
        "    lora=None,\n",
        "    controlnet=None,\n",
        "):\n",
        "    \"\"\"Loads the diffusion pipeline with optional components.\"\"\"\n",
        "    extra_kwargs = {}\n",
        "    if custom_pipeline is not None:\n",
        "        extra_kwargs[\"custom_pipeline\"] = custom_pipeline\n",
        "    if variant is not None:\n",
        "        extra_kwargs[\"variant\"] = variant\n",
        "    if dtype is not None:\n",
        "        extra_kwargs[\"torch_dtype\"] = dtype\n",
        "\n",
        "    # Handle ControlNet loading\n",
        "    if controlnet is not None:\n",
        "        from diffusers import ControlNetModel\n",
        "        try:\n",
        "            controlnet_model = ControlNetModel.from_pretrained(\n",
        "                controlnet,\n",
        "                torch_dtype=dtype,\n",
        "            )\n",
        "            extra_kwargs[\"controlnet\"] = controlnet_model\n",
        "            print(f\"Successfully loaded ControlNet: {controlnet}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load ControlNet '{controlnet}'. Error: {e}\")\n",
        "            print(\"Proceeding without ControlNet.\")\n",
        "            controlnet = None # Ensure controlnet is None if loading failed\n",
        "\n",
        "    # Handle pre-quantized models (currently onediff specific)\n",
        "    if os.path.exists(os.path.join(model_name, \"calibrate_info.txt\")):\n",
        "         # Check if QuantPipeline is available before importing\n",
        "        try:\n",
        "            from onediff.quantization import QuantPipeline\n",
        "            print(f\"Found quantization info. Loading quantized model: {model_name}\")\n",
        "            pipe = QuantPipeline.from_quantized(pipeline_cls, model_name, **extra_kwargs)\n",
        "        except ImportError:\n",
        "            print(\"Warning: `onediff.quantization.QuantPipeline` not found. Install `onediff` for quantized model support.\")\n",
        "            print(\"Loading standard pipeline instead.\")\n",
        "            pipe = pipeline_cls.from_pretrained(model_name, **extra_kwargs)\n",
        "        except Exception as e:\n",
        "             print(f\"Error loading quantized pipeline: {e}. Loading standard pipeline.\")\n",
        "             pipe = pipeline_cls.from_pretrained(model_name, **extra_kwargs)\n",
        "    else:\n",
        "        pipe = pipeline_cls.from_pretrained(model_name, **extra_kwargs)\n",
        "\n",
        "    # Set Scheduler\n",
        "    if scheduler is not None and scheduler.lower() != \"none\":\n",
        "        try:\n",
        "            scheduler_cls = getattr(importlib.import_module(\"diffusers\"), scheduler)\n",
        "            pipe.scheduler = scheduler_cls.from_config(pipe.scheduler.config)\n",
        "            print(f\"Using scheduler: {scheduler}\")\n",
        "        except (ImportError, AttributeError, Exception) as e:\n",
        "            print(f\"Warning: Failed to load or set scheduler '{scheduler}'. Using default. Error: {e}\")\n",
        "\n",
        "    # Load LoRA weights\n",
        "    if lora is not None:\n",
        "        try:\n",
        "            print(f\"Loading LoRA weights from: {lora}\")\n",
        "            pipe.load_lora_weights(lora)\n",
        "            # Optionally fuse LoRA - check diffusers version compatibility if issues arise\n",
        "            if hasattr(pipe, 'fuse_lora'):\n",
        "                 print(\"Fusing LoRA weights.\")\n",
        "                 pipe.fuse_lora()\n",
        "            else:\n",
        "                 print(\"Warning: `pipe.fuse_lora()` not found. Skipping fusion (may require newer diffusers version).\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load or fuse LoRA weights from '{lora}'. Error: {e}\")\n",
        "\n",
        "    # Disable Safety Checker if present\n",
        "    if hasattr(pipe, \"safety_checker\"):\n",
        "        pipe.safety_checker = None\n",
        "        print(\"Safety checker disabled.\")\n",
        "\n",
        "    # Move to device\n",
        "    if device is not None:\n",
        "        pipe.to(torch.device(device))\n",
        "        print(f\"Pipeline moved to device: {device}\")\n",
        "\n",
        "    return pipe\n",
        "\n",
        "class IterationProfiler:\n",
        "    \"\"\"Profiles iterations per second during pipeline steps.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.begin = None\n",
        "        self.end = None\n",
        "        self.num_iterations = 0\n",
        "        self.enabled = True # Flag to enable/disable profiling easily\n",
        "\n",
        "    def get_iter_per_sec(self):\n",
        "        if not self.enabled or self.begin is None or self.end is None or self.num_iterations == 0:\n",
        "            return None\n",
        "        try:\n",
        "            self.end.synchronize() # Ensure timing is accurate\n",
        "            dur = self.begin.elapsed_time(self.end) # Time in ms\n",
        "            if dur == 0: return float('inf') # Avoid division by zero\n",
        "            return self.num_iterations / dur * 1000.0 # Iterations per second\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error during iteration profiling: {e}\")\n",
        "            return None\n",
        "\n",
        "    def callback_on_step_end(self, pipe, i, t, callback_kwargs={}):\n",
        "        if not self.enabled:\n",
        "            return callback_kwargs\n",
        "        if torch.cuda.is_available():\n",
        "            if self.begin is None:\n",
        "                # Start timing on the first step\n",
        "                event = torch.cuda.Event(enable_timing=True)\n",
        "                event.record()\n",
        "                self.begin = event\n",
        "                self.num_iterations = 0 # Reset count at start\n",
        "            else:\n",
        "                # Record end event on subsequent steps\n",
        "                event = torch.cuda.Event(enable_timing=True)\n",
        "                event.record()\n",
        "                self.end = event\n",
        "                self.num_iterations += 1 # Increment count *after* the first step completes\n",
        "        # Pass through callback_kwargs\n",
        "        return callback_kwargs\n",
        "\n",
        "    def reset(self):\n",
        "        self.begin = None\n",
        "        self.end = None\n",
        "        self.num_iterations = 0\n",
        "\n",
        "    def set_enabled(self, enabled=True):\n",
        "        self.enabled = enabled\n",
        "        if not enabled:\n",
        "            self.reset()\n",
        "\n",
        "# --- Throughput Analysis Functions ---\n",
        "\n",
        "def calculate_inference_time_and_throughput(pipe, kwarg_inputs, n_steps, profiler):\n",
        "    \"\"\"Calculates inference time and throughput for a given number of steps.\"\"\"\n",
        "    kwarg_inputs_step = kwarg_inputs.copy()\n",
        "    kwarg_inputs_step[\"num_inference_steps\"] = n_steps\n",
        "\n",
        "    # Reset and enable profiler for this run\n",
        "    profiler.reset()\n",
        "    profiler.set_enabled(True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    # Use dummy generator for throughput test consistency if seed is None originally\n",
        "    if kwarg_inputs_step.get(\"generator\") is None:\n",
        "         kwarg_inputs_step[\"generator\"] = torch.Generator(device=\"cuda\").manual_seed(DEFAULT_SEED or 0)\n",
        "\n",
        "    _ = pipe(**kwarg_inputs_step) # Run inference\n",
        "    torch.cuda.synchronize() # Ensure completion\n",
        "    end_time = time.time()\n",
        "\n",
        "    inference_time = end_time - start_time\n",
        "    # Use profiler's calculation for it/s based on GPU events\n",
        "    iter_per_sec = profiler.get_iter_per_sec()\n",
        "    # Fallback: calculate based on wall time if profiler failed or steps < 2\n",
        "    if iter_per_sec is None and inference_time > 0 and n_steps > 0 :\n",
        "        steps_per_sec_wall = n_steps / inference_time\n",
        "    else:\n",
        "        steps_per_sec_wall = iter_per_sec if iter_per_sec is not None else 0\n",
        "\n",
        "    # Disable profiler after use\n",
        "    profiler.set_enabled(False)\n",
        "\n",
        "    return inference_time, steps_per_sec_wall\n",
        "\n",
        "\n",
        "def generate_data_and_fit_model(pipe, base_kwarg_inputs, steps_range, profiler):\n",
        "    \"\"\"Generates throughput data across a range of steps and fits a linear model.\"\"\"\n",
        "    print(\"\\n--- Starting Throughput Analysis ---\")\n",
        "    data = {\"steps\": [], \"inference_time\": [], \"throughput\": []}\n",
        "    height = base_kwarg_inputs.get('height', 512)\n",
        "    width = base_kwarg_inputs.get('width', 512)\n",
        "\n",
        "    for n_steps in steps_range:\n",
        "        if n_steps <= 0: continue # Skip invalid step counts\n",
        "        print(f\"Testing {n_steps} steps...\")\n",
        "        try:\n",
        "            inference_time, throughput = calculate_inference_time_and_throughput(\n",
        "                pipe, base_kwarg_inputs, n_steps, profiler\n",
        "            )\n",
        "            data[\"steps\"].append(n_steps)\n",
        "            data[\"inference_time\"].append(inference_time)\n",
        "            # Store throughput (steps/sec)\n",
        "            data[\"throughput\"].append(throughput)\n",
        "            print(\n",
        "                f\"  Steps: {n_steps}, Inference Time: {inference_time:.3f}s, Throughput: {throughput:.3f} steps/s\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  Error during {n_steps} steps run: {e}\")\n",
        "            # Optionally break or continue\n",
        "            # break\n",
        "            continue\n",
        "        # Short sleep to allow GPU cool-down if needed, can be removed\n",
        "        # time.sleep(0.5)\n",
        "\n",
        "\n",
        "    if not data[\"steps\"] or len(data[\"steps\"]) < 2 :\n",
        "        print(\"Insufficient data points for throughput modeling.\")\n",
        "        return None, None\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Calculate Average Throughput from collected data\n",
        "    # Exclude potential outliers (e.g., first few runs if warmup wasn't enough, or very low step counts)\n",
        "    # Simple approach: exclude first point or use median/trimmed mean\n",
        "    valid_throughputs = [t for t in data[\"throughput\"] if t > 0 and np.isfinite(t)]\n",
        "    if not valid_throughputs:\n",
        "         print(\"No valid throughput measurements recorded.\")\n",
        "         average_throughput = 0\n",
        "    else:\n",
        "        average_throughput = np.mean(valid_throughputs) # Or np.median(valid_throughputs)\n",
        "        print(f\"\\nAverage Measured Throughput: {average_throughput:.3f} steps/s\")\n",
        "\n",
        "\n",
        "    # Fit linear model: time = slope * steps + intercept\n",
        "    # Requires at least 2 data points\n",
        "    try:\n",
        "        coefficients = np.polyfit(df[\"steps\"], df[\"inference_time\"], 1)\n",
        "        slope = coefficients[0]\n",
        "        intercept = coefficients[1]\n",
        "        print(f\"Linear Model Fit: Time = {slope:.4f} * Steps + {intercept:.4f}\")\n",
        "\n",
        "        # Estimate throughput based on the slope (time per step)\n",
        "        if slope > 1e-9: # Avoid division by zero or near-zero\n",
        "            throughput_from_slope = 1.0 / slope\n",
        "            print(f\"Throughput estimated from slope (ignoring base cost): {throughput_from_slope:.3f} steps/s\")\n",
        "        else:\n",
        "            print(\"Slope is too small to estimate throughput reliably.\")\n",
        "            throughput_from_slope = None\n",
        "\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        print(f\"Could not fit linear model to data: {e}\")\n",
        "        coefficients = None\n",
        "        throughput_from_slope = None\n",
        "\n",
        "\n",
        "    print(\"--- Throughput Analysis Complete ---\")\n",
        "    return data, coefficients\n",
        "\n",
        "\n",
        "def plot_data_and_model(data, coefficients):\n",
        "    \"\"\"Plots the inference time vs steps and the fitted linear model.\"\"\"\n",
        "    if data is None or not data[\"steps\"]:\n",
        "        print(\"No data to plot.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(data[\"steps\"], data[\"inference_time\"], color=\"blue\", label=\"Measured Data\")\n",
        "\n",
        "    if coefficients is not None and len(coefficients) == 2:\n",
        "        steps_line = np.array(data[\"steps\"])\n",
        "        time_line = np.polyval(coefficients, steps_line)\n",
        "        plt.plot(steps_line, time_line, color=\"red\", label=f\"Fit: Time = {coefficients[0]:.4f}*Steps + {coefficients[1]:.4f}\")\n",
        "        plt.legend()\n",
        "\n",
        "    plt.title(\"Inference Time vs. Number of Steps\")\n",
        "    plt.xlabel(\"Number of Inference Steps\")\n",
        "    plt.ylabel(\"Inference Time (seconds)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save or show the plot\n",
        "    plot_filename = \"throughput_analysis.png\"\n",
        "    try:\n",
        "        plt.savefig(plot_filename)\n",
        "        print(f\"Throughput plot saved to {plot_filename}\")\n",
        "        # plt.show() # Uncomment to display interactively if not in a headless environment\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving/showing plot: {e}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    # --- Determine Task and Pipeline Class ---\n",
        "    effective_task = args.task\n",
        "    if effective_task == \"auto\":\n",
        "        if args.input_image is not None:\n",
        "            effective_task = \"image2image\"\n",
        "            print(\"Detected Image-to-Image task (input image provided).\")\n",
        "        else:\n",
        "            effective_task = \"text2image\"\n",
        "            print(\"Detected Text-to-Image task (no input image provided).\")\n",
        "\n",
        "    pipeline_cls = None\n",
        "    if effective_task == \"text2image\":\n",
        "        if args.deepcache:\n",
        "             try:\n",
        "                 # Requires onediffx.deep_cache module\n",
        "                 from onediffx.deep_cache import StableDiffusionXLPipeline as PipelineForT2IDeepCache\n",
        "                 from onediffx.deep_cache import StableDiffusionPipeline as SD15PipelineForT2IDeepCache # Example for SD 1.5\n",
        "                 # Basic check if model name implies SDXL\n",
        "                 if \"xl\" in args.model.lower():\n",
        "                     pipeline_cls = PipelineForT2IDeepCache\n",
        "                     print(\"Using DeepCache SDXL Text-to-Image pipeline.\")\n",
        "                 else:\n",
        "                     # Assuming SD 1.5/2.1 for non-XL, adjust if needed\n",
        "                     pipeline_cls = SD15PipelineForT2IDeepCache\n",
        "                     print(\"Using DeepCache Stable Diffusion Text-to-Image pipeline.\")\n",
        "\n",
        "             except ImportError:\n",
        "                 print(\"Warning: DeepCache pipelines not found in onediffx. Falling back to standard AutoPipeline.\")\n",
        "                 from diffusers import AutoPipelineForText2Image\n",
        "                 pipeline_cls = AutoPipelineForText2Image\n",
        "                 args.deepcache = False # Disable deepcache if import failed\n",
        "        else:\n",
        "            from diffusers import AutoPipelineForText2Image\n",
        "            pipeline_cls = AutoPipelineForText2Image\n",
        "            print(\"Using AutoPipelineForText2Image.\")\n",
        "\n",
        "    elif effective_task == \"image2image\":\n",
        "        # For general I2I, AutoPipelineForImage2Image is suitable\n",
        "        from diffusers import AutoPipelineForImage2Image\n",
        "        pipeline_cls = AutoPipelineForImage2Image\n",
        "        print(\"Using AutoPipelineForImage2Image.\")\n",
        "\n",
        "    elif effective_task == \"instructpix2pix\":\n",
        "        if args.input_image is None:\n",
        "            raise ValueError(\"--input-image is required for the 'instructpix2pix' task.\")\n",
        "        # Check if the specified model is likely InstructPix2Pix\n",
        "        if \"instruct-pix2pix\" not in args.model.lower():\n",
        "             print(f\"Warning: Model '{args.model}' might not be an InstructPix2Pix model, but task='instructpix2pix' was specified.\")\n",
        "        from diffusers import StableDiffusionInstructPix2PixPipeline\n",
        "        pipeline_cls = StableDiffusionInstructPix2PixPipeline\n",
        "        print(\"Using StableDiffusionInstructPix2PixPipeline.\")\n",
        "        # InstructPix2Pix often uses specific default prompts\n",
        "        if args.prompt == DEFAULT_PROMPT: # If user didn't override prompt\n",
        "            args.prompt = \"apply the instruction to the image\" # More suitable default for instruct-pix2pix\n",
        "            print(f\"Using InstructPix2Pix specific default prompt: '{args.prompt}'\")\n",
        "\n",
        "    else:\n",
        "        # This case should not be reached due to argparse choices\n",
        "         raise ValueError(f\"Invalid task specified: {args.task}\")\n",
        "\n",
        "    if pipeline_cls is None:\n",
        "         raise RuntimeError(\"Could not determine the pipeline class. Check task and model.\")\n",
        "\n",
        "\n",
        "    # --- Load Pipeline ---\n",
        "    print(f\"\\nLoading model: {args.model}\")\n",
        "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32 # Use float16 on CUDA by default\n",
        "    pipe = load_pipe(\n",
        "        pipeline_cls,\n",
        "        args.model,\n",
        "        variant=args.variant,\n",
        "        custom_pipeline=args.custom_pipeline,\n",
        "        scheduler=args.scheduler,\n",
        "        lora=args.lora,\n",
        "        controlnet=args.controlnet,\n",
        "        dtype=dtype,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    )\n",
        "\n",
        "    # --- Determine Optimal Height/Width ---\n",
        "    # Use provided H/W if set, otherwise try to infer from model\n",
        "    height = args.height\n",
        "    width = args.width\n",
        "    if height is None or width is None:\n",
        "        try:\n",
        "            model_height = pipe.unet.config.sample_size * pipe.vae_scale_factor\n",
        "            model_width = pipe.unet.config.sample_size * pipe.vae_scale_factor\n",
        "            if height is None: height = model_height\n",
        "            if width is None: width = model_width\n",
        "            print(f\"Auto-detected resolution: {height}x{width}\")\n",
        "        except AttributeError:\n",
        "            # Fallback if detection fails (e.g., non-standard pipeline structure)\n",
        "            fallback_res = 512 if \"xl\" not in args.model.lower() else 1024\n",
        "            if height is None: height = fallback_res\n",
        "            if width is None: width = fallback_res\n",
        "            print(f\"Warning: Could not auto-detect resolution. Using default: {height}x{width}\")\n",
        "\n",
        "    # Ensure height and width are multiples of VAE scale factor (usually 8)\n",
        "    vae_scale_factor = getattr(pipe, \"vae_scale_factor\", 8)\n",
        "    height = (height // vae_scale_factor) * vae_scale_factor\n",
        "    width = (width // vae_scale_factor) * vae_scale_factor\n",
        "    if args.height != height or args.width != width:\n",
        "         print(f\"Adjusted resolution to be multiples of {vae_scale_factor}: {height}x{width}\")\n",
        "\n",
        "\n",
        "    # --- Apply Compiler/Quantization ---\n",
        "    compiled = False\n",
        "    if args.compiler != \"none\" and torch.cuda.is_available():\n",
        "        print(f\"\\nApplying compiler: {args.compiler}\")\n",
        "        if args.compiler == \"oneflow\":\n",
        "            # Requires oneflow and onediff to be installed\n",
        "            try:\n",
        "                import oneflow # Check if oneflow is installed\n",
        "                pipe = compile_pipe(pipe, backend=\"oneflow\") # Assumes compile_pipe handles backend selection\n",
        "                compiled = True\n",
        "                print(\"Oneflow backend via compile_pipe is active.\")\n",
        "            except ImportError:\n",
        "                print(\"Warning: OneFlow not installed. Skipping OneFlow compilation.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during OneFlow compilation: {e}. Proceeding without compilation.\")\n",
        "\n",
        "        elif args.compiler == \"nexfort\":\n",
        "            # Requires nexfort, torchao, onediffx\n",
        "            try:\n",
        "                quantize_options = {}\n",
        "                if args.quantize:\n",
        "                    print(\"Applying Nexfort quantization...\")\n",
        "                    if args.quantize_config:\n",
        "                        try:\n",
        "                            quantize_options = json.loads(args.quantize_config)\n",
        "                            print(f\"Using custom quantize config: {quantize_options}\")\n",
        "                        except json.JSONDecodeError:\n",
        "                            print(f\"Warning: Invalid JSON in --quantize-config. Using default.\")\n",
        "                            quantize_options = {\"quant_type\": \"fp8_e4m3_e4m3_dynamic\"} # Default FP8\n",
        "                    else:\n",
        "                         quantize_options = {\"quant_type\": \"fp8_e4m3_e4m3_dynamic\"} # Default FP8\n",
        "                         print(f\"Using default quantize config: {quantize_options}\")\n",
        "\n",
        "                    if args.quant_submodules_config_path:\n",
        "                         print(f\"Using quant submodules config: {args.quant_submodules_config_path}\")\n",
        "                         pipe = quantize_pipe(\n",
        "                             pipe,\n",
        "                             quant_submodules_config_path=args.quant_submodules_config_path,\n",
        "                             ignores=[], # Example: Add submodules to ignore if needed\n",
        "                              **quantize_options\n",
        "                         )\n",
        "                    else:\n",
        "                         pipe = quantize_pipe(pipe, ignores=[], **quantize_options)\n",
        "                    print(\"Quantization applied.\")\n",
        "\n",
        "\n",
        "                compiler_options = {}\n",
        "                if args.compiler_config:\n",
        "                    try:\n",
        "                        compiler_options = json.loads(args.compiler_config)\n",
        "                        print(f\"Using custom compiler config: {compiler_options}\")\n",
        "                    except json.JSONDecodeError:\n",
        "                         print(f\"Warning: Invalid JSON in --compiler-config. Using default.\")\n",
        "                         # Safe default options string\n",
        "                         compiler_options = {\"mode\": \"max-optimize:max-autotune:freezing\", \"memory_format\": \"channels_last\"}\n",
        "                else:\n",
        "                    # Safe default options string\n",
        "                    compiler_options = {\"mode\": \"max-optimize:max-autotune:freezing\", \"memory_format\": \"channels_last\"}\n",
        "                    print(f\"Using default compiler config: {compiler_options}\")\n",
        "\n",
        "                # Apply compilation\n",
        "                pipe = compile_pipe(\n",
        "                    pipe,\n",
        "                    backend=\"nexfort\",\n",
        "                    options=compiler_options,\n",
        "                    fuse_qkv_projections=True # Generally safe and beneficial\n",
        "                )\n",
        "                compiled = True\n",
        "                print(\"Nexfort backend is active.\")\n",
        "\n",
        "            except ImportError as e:\n",
        "                 print(f\"Warning: Missing dependencies for nexfort ({e}). Skipping nexfort compilation.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Error during Nexfort setup: {e}. Proceeding without nexfort.\")\n",
        "\n",
        "\n",
        "        elif args.compiler in (\"compile\", \"compile-max-autotune\"):\n",
        "             # Uses torch.compile\n",
        "            mode = \"max-autotune\" if args.compiler == \"compile-max-autotune\" else None\n",
        "            print(f\"Applying torch.compile (mode: {mode or 'default'})...\")\n",
        "            # Compile relevant components\n",
        "            compiled_components = []\n",
        "            for component_name in [\"unet\", \"vae\", \"transformer\", \"controlnet\"]:\n",
        "                 if hasattr(pipe, component_name) and getattr(pipe, component_name) is not None:\n",
        "                     try:\n",
        "                         print(f\"Compiling {component_name}...\")\n",
        "                         setattr(pipe, component_name, torch.compile(getattr(pipe, component_name), mode=mode))\n",
        "                         compiled_components.append(component_name)\n",
        "                     except Exception as e:\n",
        "                         print(f\"Warning: Failed to compile {component_name}. Error: {e}\")\n",
        "\n",
        "            if compiled_components:\n",
        "                 print(f\"Successfully compiled: {', '.join(compiled_components)}\")\n",
        "                 compiled = True\n",
        "            else:\n",
        "                 print(\"No components were compiled with torch.compile.\")\n",
        "\n",
        "        else:\n",
        "             # Should not happen due to argparse choices\n",
        "             print(f\"Warning: Unknown compiler '{args.compiler}' requested. Running in eager mode.\")\n",
        "    elif args.compiler != \"none\":\n",
        "        print(\"CUDA not available, skipping compilation.\")\n",
        "\n",
        "\n",
        "    # --- Load Images (Input and Control) ---\n",
        "    input_image = None\n",
        "    if args.input_image:\n",
        "        print(f\"Loading input image: {args.input_image}\")\n",
        "        try:\n",
        "            input_image = load_image(args.input_image)\n",
        "            input_image = input_image.resize((width, height), Image.LANCZOS)\n",
        "            print(f\"Input image resized to {width}x{height}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or resizing input image: {e}. Cannot perform Image-to-Image task.\")\n",
        "            return # Exit if I2I is required but image loading fails\n",
        "\n",
        "    control_image = None\n",
        "    if args.control_image:\n",
        "        if args.controlnet is None:\n",
        "             print(\"Warning: --control-image provided but no --controlnet model specified. Control image will be ignored.\")\n",
        "        else:\n",
        "            print(f\"Loading control image: {args.control_image}\")\n",
        "            try:\n",
        "                control_image = load_image(args.control_image)\n",
        "                control_image = control_image.resize((width, height), Image.LANCZOS)\n",
        "                print(f\"Control image resized to {width}x{height}\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Warning: Error loading or resizing control image: {e}. Proceeding without control image.\")\n",
        "                 control_image = None # Ensure it's None if loading fails\n",
        "    elif args.controlnet is not None and input_image is not None:\n",
        "        # If controlnet is specified but no specific control image, use the input image as control\n",
        "        print(\"Using input image as control image for ControlNet.\")\n",
        "        control_image = input_image\n",
        "    elif args.controlnet is not None:\n",
        "        # ControlNet specified but no control image and no input image (T2I mode)\n",
        "        # Generate a dummy control image (e.g., blank) or raise error?\n",
        "        # Let's create a blank white image as a placeholder\n",
        "        print(\"Warning: ControlNet specified but no --control-image or --input-image provided.\")\n",
        "        print(f\"Creating a blank white control image ({width}x{height}).\")\n",
        "        control_image = Image.new(\"RGB\", (width, height), (255, 255, 255))\n",
        "        # Or could raise error: raise ValueError(\"ControlNet requires --control-image or --input-image.\")\n",
        "\n",
        "\n",
        "    # --- Prepare Keyword Arguments for Pipeline Call ---\n",
        "    def get_kwarg_inputs(current_args, current_height, current_width, current_input_image, current_control_image):\n",
        "        kwarg_inputs = dict(\n",
        "            prompt=current_args.prompt,\n",
        "            negative_prompt=current_args.negative_prompt,\n",
        "            height=current_height,\n",
        "            width=current_width,\n",
        "            num_images_per_prompt=current_args.batch,\n",
        "            num_inference_steps=current_args.steps, # Ensure steps are included\n",
        "            generator=(\n",
        "                None\n",
        "                if current_args.seed is None\n",
        "                else torch.Generator(device=\"cuda\" if torch.cuda.is_available() else \"cpu\").manual_seed(current_args.seed)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Add image for I2I tasks\n",
        "        if effective_task in [\"image2image\", \"instructpix2pix\"] and current_input_image is not None:\n",
        "            kwarg_inputs[\"image\"] = current_input_image\n",
        "        elif effective_task in [\"image2image\", \"instructpix2pix\"]:\n",
        "             # Should have been caught earlier, but double-check\n",
        "             raise RuntimeError(f\"Input image is required for task '{effective_task}' but is missing.\")\n",
        "\n",
        "\n",
        "        # Add control image if available and ControlNet is loaded\n",
        "        if current_control_image is not None and hasattr(pipe, 'controlnet') and pipe.controlnet is not None:\n",
        "             # Some pipelines expect 'control_image', others 'image' if it's the primary input\n",
        "             # Check signature - this is complex, maybe rely on AutoPipeline or specific pipeline needs\n",
        "             # Simple approach: Add 'control_image' if ControlNet is present.\n",
        "             # If it conflicts with 'image', the specific pipeline should handle it or error out.\n",
        "             kwarg_inputs[\"control_image\"] = current_control_image\n",
        "             # For T2I + ControlNet, sometimes 'image' needs to be the control image\n",
        "             if effective_task == \"text2image\" and \"image\" not in kwarg_inputs:\n",
        "                  kwarg_inputs[\"image\"] = current_control_image # Pass control image as 'image' for T2I ControlNet\n",
        "                  print(\"Passing control image as 'image' argument for T2I + ControlNet task.\")\n",
        "\n",
        "\n",
        "        # Add DeepCache arguments if enabled\n",
        "        if current_args.deepcache and effective_task == \"text2image\": # Currently example DeepCache pipeline is T2I\n",
        "            # Check if the pipeline actually supports these args (might need more robust check)\n",
        "            sig = inspect.signature(pipe.__call__)\n",
        "            if \"cache_interval\" in sig.parameters:\n",
        "                kwarg_inputs[\"cache_interval\"] = current_args.cache_interval\n",
        "                kwarg_inputs[\"cache_layer_id\"] = current_args.cache_layer_id\n",
        "                kwarg_inputs[\"cache_block_id\"] = current_args.cache_block_id\n",
        "            else:\n",
        "                print(\"Warning: --deepcache specified, but pipeline does not seem to support cache arguments. Disabling.\")\n",
        "                args.deepcache = False # Disable if not supported\n",
        "\n",
        "        # Add extra keyword arguments from JSON string\n",
        "        if current_args.extra_call_kwargs:\n",
        "            try:\n",
        "                extra_kwargs = json.loads(current_args.extra_call_kwargs)\n",
        "                # Filter out args already handled explicitly to avoid conflicts\n",
        "                keys_to_remove = {\"prompt\", \"negative_prompt\", \"height\", \"width\", \"num_images_per_prompt\", \"generator\", \"num_inference_steps\", \"image\", \"control_image\", \"cache_interval\", \"cache_layer_id\", \"cache_block_id\", \"callback_on_step_end\", \"callback\"}\n",
        "                filtered_extra_kwargs = {k: v for k, v in extra_kwargs.items() if k not in keys_to_remove}\n",
        "\n",
        "                # Check for potential conflicts (e.g., guidance_scale vs EtaDDIM) - let diffusers handle it mostly\n",
        "                print(f\"Adding extra call arguments: {filtered_extra_kwargs}\")\n",
        "                kwarg_inputs.update(filtered_extra_kwargs)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Warning: Invalid JSON in --extra-call-kwargs: {e}. Ignoring extra args.\")\n",
        "\n",
        "        return kwarg_inputs\n",
        "\n",
        "    # --- Warmup Runs ---\n",
        "    if args.warmups > 0:\n",
        "        print(\"\\n=======================================\")\n",
        "        print(f\"Begin warmup ({args.warmups} runs)...\")\n",
        "        # Use a temporary profiler for warmup that's disabled\n",
        "        warmup_profiler = IterationProfiler()\n",
        "        warmup_profiler.set_enabled(False)\n",
        "        warmup_kwarg_inputs = get_kwarg_inputs(args, height, width, input_image, control_image)\n",
        "        # Add dummy callback if needed by signature, but disabled profiler won't use it\n",
        "        sig = inspect.signature(pipe.__call__)\n",
        "        if \"callback_on_step_end\" in sig.parameters:\n",
        "             warmup_kwarg_inputs[\"callback_on_step_end\"] = warmup_profiler.callback_on_step_end\n",
        "        elif \"callback\" in sig.parameters: # Older diffusers convention\n",
        "             warmup_kwarg_inputs[\"callback\"] = warmup_profiler.callback_on_step_end\n",
        "\n",
        "        start_warmup_time = time.time()\n",
        "        for i in range(args.warmups):\n",
        "            # Ensure generator is reset/new for each warmup if seed is None\n",
        "            current_seed = args.seed if args.seed is not None else int(time.time()) + i\n",
        "            warmup_kwarg_inputs[\"generator\"] = torch.Generator(device=\"cuda\" if torch.cuda.is_available() else \"cpu\").manual_seed(current_seed)\n",
        "            _ = pipe(**warmup_kwarg_inputs)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        end_warmup_time = time.time()\n",
        "        print(\"End warmup\")\n",
        "        print(f\"Warmup time: {end_warmup_time - start_warmup_time:.3f}s\")\n",
        "        print(\"=======================================\")\n",
        "        del warmup_profiler # Clean up\n",
        "\n",
        "    # --- Timed Inference Run ---\n",
        "    print(\"\\n=======================================\")\n",
        "    print(\"Begin timed inference run...\")\n",
        "    iter_profiler = IterationProfiler()\n",
        "    # Ensure profiler is enabled for the main run\n",
        "    iter_profiler.set_enabled(torch.cuda.is_available()) # Only enable if CUDA is available for timing events\n",
        "\n",
        "    kwarg_inputs = get_kwarg_inputs(args, height, width, input_image, control_image)\n",
        "\n",
        "    # Add the profiling callback\n",
        "    sig = inspect.signature(pipe.__call__)\n",
        "    if \"callback_on_step_end\" in sig.parameters:\n",
        "        kwarg_inputs[\"callback_on_step_end\"] = iter_profiler.callback_on_step_end\n",
        "        print(\"Iteration profiler attached via callback_on_step_end.\")\n",
        "    elif \"callback\" in sig.parameters: # Older diffusers convention\n",
        "        kwarg_inputs[\"callback\"] = iter_profiler.callback_on_step_end\n",
        "        print(\"Iteration profiler attached via callback.\")\n",
        "    else:\n",
        "        iter_profiler.set_enabled(False) # Disable profiler if no callback mechanism found\n",
        "        print(\"Warning: Pipeline does not support step callbacks. Iteration profiling disabled.\")\n",
        "\n",
        "\n",
        "    # Clear CUDA cache before timed run (optional, might help consistency)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        initial_mem = torch.cuda.max_memory_allocated() / (1024**3) # GiB\n",
        "\n",
        "    start_time = time.time()\n",
        "    output_images = pipe(**kwarg_inputs).images\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize() # Wait for GPU to finish\n",
        "    end_time = time.time()\n",
        "\n",
        "    inference_time = end_time - start_time\n",
        "    print(\"Inference complete.\")\n",
        "    print(\"=======================================\")\n",
        "    print(f\"Task Type: {effective_task.upper()}\")\n",
        "    print(f\"Model: {args.model}\")\n",
        "    if compiled: print(f\"Compiler: {args.compiler}\")\n",
        "    if args.quantize: print(\"Quantization: Enabled (Nexfort)\")\n",
        "    print(f\"Resolution: {height}x{width}\")\n",
        "    print(f\"Steps: {args.steps}\")\n",
        "    print(f\"Inference Time (Wall Clock): {inference_time:.3f}s\")\n",
        "\n",
        "    # Report Iterations Per Second from profiler\n",
        "    iter_per_sec = iter_profiler.get_iter_per_sec()\n",
        "    if iter_per_sec is not None:\n",
        "        print(f\"Iterations Per Second (GPU Profiled): {iter_per_sec:.3f}\")\n",
        "    elif iter_profiler.enabled and args.steps > 1:\n",
        "         print(\"Iterations Per Second (GPU Profiled): N/A (profiling error or too few steps)\")\n",
        "    elif inference_time > 0 and args.steps > 0:\n",
        "        # Fallback to wall clock steps/sec if profiler not available/failed\n",
        "        wall_steps_per_sec = args.steps / inference_time\n",
        "        print(f\"Steps Per Second (Wall Clock): {wall_steps_per_sec:.3f}\")\n",
        "\n",
        "\n",
        "    # Report Memory Usage\n",
        "    if torch.cuda.is_available():\n",
        "        # Use torch.cuda.max_memory_allocated() which tracks peak usage\n",
        "        cuda_mem_after_used = torch.cuda.max_memory_allocated() / (1024**3) # GiB\n",
        "        # Reset peak stats for next potential runs if needed\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        print(f\"Max CUDA Memory Used: {cuda_mem_after_used:.3f} GiB\")\n",
        "    # elif args.compiler == \"oneflow\": # Specific check for oneflow if needed\n",
        "    #     try:\n",
        "    #         import oneflow as flow\n",
        "    #         # Note: OneFlow's memory reporting might differ\n",
        "    #         cuda_mem_after_used = flow._oneflow_internal.GetCUDAMemoryUsed() / 1024 # KiB? Check unit\n",
        "    #         print(f\"Max used OneFlow CUDA memory : {cuda_mem_after_used:.3f} Units (check OneFlow docs for unit)\")\n",
        "    #     except ImportError:\n",
        "    #         pass # Oneflow not installed\n",
        "    print(\"=======================================\")\n",
        "\n",
        "    # --- Save Output Image ---\n",
        "    if args.output_image:\n",
        "        try:\n",
        "            output_images[0].save(args.output_image)\n",
        "            print(f\"Output image saved to: {args.output_image}\")\n",
        "        except IndexError:\n",
        "            print(\"Error: No images generated.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving output image to {args.output_image}: {e}\")\n",
        "    else:\n",
        "        print(\"No --output-image path specified. Image not saved.\")\n",
        "\n",
        "    # --- Optional: Run Multiple Resolutions Test ---\n",
        "    if args.run_multiple_resolutions:\n",
        "        print(\"\\n--- Testing Multiple Resolutions ---\")\n",
        "        sizes = [1024, 768, 512, 256] # Example sizes\n",
        "        base_kwarg_inputs = get_kwarg_inputs(args, height, width, input_image, control_image)\n",
        "        # Remove callbacks for these runs if they were added\n",
        "        base_kwarg_inputs.pop(\"callback_on_step_end\", None)\n",
        "        base_kwarg_inputs.pop(\"callback\", None)\n",
        "\n",
        "        for h in sizes:\n",
        "            for w in sizes:\n",
        "                 # Skip if same as original run\n",
        "                 if h == height and w == width: continue\n",
        "\n",
        "                 # Adjust resolution, ensuring it's valid (multiple of 8)\n",
        "                 h_test = (h // vae_scale_factor) * vae_scale_factor\n",
        "                 w_test = (w // vae_scale_factor) * vae_scale_factor\n",
        "                 if h_test == 0 or w_test == 0: continue # Skip invalid zero sizes\n",
        "\n",
        "                 print(f\"Running at resolution: {h_test}x{w_test}\")\n",
        "                 current_kwarg_inputs = base_kwarg_inputs.copy()\n",
        "                 current_kwarg_inputs[\"height\"] = h_test\n",
        "                 current_kwarg_inputs[\"width\"] = w_test\n",
        "\n",
        "                 # Need to resize input/control images if they exist for I2I\n",
        "                 current_input_image_test = None\n",
        "                 if input_image:\n",
        "                      try:\n",
        "                          current_input_image_test = input_image.resize((w_test, h_test), Image.LANCZOS)\n",
        "                          if effective_task in [\"image2image\", \"instructpix2pix\"]:\n",
        "                               current_kwarg_inputs[\"image\"] = current_input_image_test\n",
        "                      except Exception as e:\n",
        "                          print(f\"  Warn: Failed to resize input image for {h_test}x{w_test}. Skipping.\")\n",
        "                          continue\n",
        "\n",
        "                 current_control_image_test = None\n",
        "                 if control_image:\n",
        "                      try:\n",
        "                          current_control_image_test = control_image.resize((w_test, h_test), Image.LANCZOS)\n",
        "                          if \"control_image\" in current_kwarg_inputs: # Check if key exists\n",
        "                               current_kwarg_inputs[\"control_image\"] = current_control_image_test\n",
        "                          if effective_task == \"text2image\" and args.controlnet: # T2I+ControlNet case\n",
        "                              current_kwarg_inputs[\"image\"] = current_control_image_test\n",
        "                      except Exception as e:\n",
        "                           print(f\"  Warn: Failed to resize control image for {h_test}x{w_test}. Skipping.\")\n",
        "                           continue\n",
        "\n",
        "\n",
        "                 # Reset generator for consistency if seed is None\n",
        "                 if args.seed is None:\n",
        "                     current_kwarg_inputs[\"generator\"] = torch.Generator(device=\"cuda\" if torch.cuda.is_available() else \"cpu\").manual_seed(DEFAULT_SEED or 0) # Use default seed for test runs\n",
        "\n",
        "\n",
        "                 try:\n",
        "                     start_res_time = time.time()\n",
        "                     _ = pipe(**current_kwarg_inputs).images\n",
        "                     if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "                     end_res_time = time.time()\n",
        "                     print(f\"  Inference time: {end_res_time - start_res_time:.3f} seconds\")\n",
        "                 except Exception as e:\n",
        "                     print(f\"  Error during {h_test}x{w_test} run: {e}\")\n",
        "                 # Optional: small delay\n",
        "                 # time.sleep(0.5)\n",
        "        print(\"--- Multi-resolution Testing Complete ---\")\n",
        "\n",
        "\n",
        "    # --- Optional: Throughput Analysis ---\n",
        "    if args.throughput:\n",
        "        # Use a range starting from a few steps up to maybe slightly more than default\n",
        "        steps_range = range(5, max(55, args.steps + 15), 5) # e.g., 5, 10, 15... up to 50 or more\n",
        "        base_kwarg_inputs = get_kwarg_inputs(args, height, width, input_image, control_image)\n",
        "        # Remove callbacks for throughput runs as we use a dedicated profiler inside\n",
        "        base_kwarg_inputs.pop(\"callback_on_step_end\", None)\n",
        "        base_kwarg_inputs.pop(\"callback\", None)\n",
        "\n",
        "        throughput_data, throughput_coeffs = generate_data_and_fit_model(\n",
        "            pipe, base_kwarg_inputs, steps_range, iter_profiler # Reuse main profiler object\n",
        "            )\n",
        "        if throughput_data:\n",
        "             plot_data_and_model(throughput_data, throughput_coeffs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RknivSweOxAX",
        "outputId": "999a2a0e-a8fc-4171-c171-5e077cfa7f5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing generate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile templates/index.html\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>DreamDoodle - AI Image Generation</title>\n",
        "    <!-- Tailwind CSS via CDN -->\n",
        "    <script src=\"https://cdn.tailwindcss.com\"></script>\n",
        "    <!-- Font Awesome via CDN -->\n",
        "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css\">\n",
        "    <style>\n",
        "        @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap');\n",
        "\n",
        "        body {\n",
        "            font-family: 'Poppins', sans-serif;\n",
        "            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n",
        "            min-height: 100vh;\n",
        "            display: flex; /* Use flexbox for footer */\n",
        "            flex-direction: column; /* Stack elements vertically */\n",
        "        }\n",
        "\n",
        "        .gradient-text {\n",
        "            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
        "            -webkit-background-clip: text;\n",
        "            background-clip: text;\n",
        "            color: transparent;\n",
        "        }\n",
        "\n",
        "        .image-preview {\n",
        "            transition: all 0.3s ease;\n",
        "            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);\n",
        "        }\n",
        "\n",
        "        .image-preview:hover {\n",
        "            transform: translateY(-2px);\n",
        "            box-shadow: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);\n",
        "        }\n",
        "\n",
        "        .upload-area {\n",
        "            border: 2px dashed #cbd5e0;\n",
        "            transition: all 0.3s ease;\n",
        "        }\n",
        "\n",
        "        .upload-area:hover {\n",
        "            border-color: #667eea;\n",
        "            background-color: rgba(102, 126, 234, 0.05);\n",
        "        }\n",
        "\n",
        "        .upload-area.dragover {\n",
        "            border-color: #667eea;\n",
        "            background-color: rgba(102, 126, 234, 0.1);\n",
        "        }\n",
        "\n",
        "        .generated-image {\n",
        "            animation: fadeIn 0.5s ease-in-out;\n",
        "        }\n",
        "\n",
        "        @keyframes fadeIn {\n",
        "            from { opacity: 0; transform: scale(0.95); }\n",
        "            to { opacity: 1; transform: scale(1); }\n",
        "        }\n",
        "\n",
        "        /* --- Styling for Flask Error Messages --- */\n",
        "        .flask-error {\n",
        "             background-color: #f8d7da; color: #721c24; border: 1px solid #f5c6cb;\n",
        "             margin-top: 20px; padding: 15px; border-radius: 8px; text-align: center;\n",
        "        }\n",
        "        .flask-error pre {\n",
        "             white-space: pre-wrap; word-wrap: break-word; text-align: left; font-family: monospace;\n",
        "             font-size: 0.9em; max-height: 200px; overflow-y: auto; background-color: #f1f1f1;\n",
        "             padding: 10px; border-radius: 3px; margin-top: 10px;\n",
        "         }\n",
        "\n",
        "        /* --- Styling for Client-Side Toast --- */\n",
        "        .toast-container {\n",
        "            position: fixed;\n",
        "            bottom: 1rem;\n",
        "            right: 1rem;\n",
        "            z-index: 50;\n",
        "        }\n",
        "        .toast {\n",
        "            animation: slideIn 0.3s ease-out, fadeOut 0.5s ease-out 2.5s forwards;\n",
        "            display: flex;\n",
        "            align-items: center;\n",
        "            padding: 0.75rem 1rem; /* py-3 px-4 */\n",
        "            border-radius: 0.5rem; /* rounded-lg */\n",
        "            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); /* shadow-lg */\n",
        "            background-color: #ef4444; /* bg-red-500 */\n",
        "            color: white;\n",
        "        }\n",
        "         .toast i { margin-right: 0.5rem; /* mr-2 */}\n",
        "\n",
        "        @keyframes slideIn {\n",
        "            from { transform: translateY(20px); opacity: 0; }\n",
        "            to { transform: translateY(0); opacity: 1; }\n",
        "        }\n",
        "        @keyframes fadeOut {\n",
        "            from { opacity: 1; }\n",
        "            to { opacity: 0; }\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body class=\"min-h-screen flex flex-col\">\n",
        "    <!-- Header -->\n",
        "    <header class=\"bg-white shadow-sm py-4\">\n",
        "        <div class=\"container mx-auto px-4\">\n",
        "            <div class=\"flex justify-between items-center\">\n",
        "                <h1 class=\"text-3xl font-bold gradient-text\">DreamDoodle</h1>\n",
        "                <div class=\"flex items-center space-x-4\">\n",
        "                    <!-- Sign in button is just decorative for now -->\n",
        "                    <button class=\"px-4 py-2 bg-gradient-to-r from-purple-500 to-indigo-600 text-white rounded-lg hover:opacity-90 transition\">\n",
        "                        <i class=\"fas fa-sign-in-alt mr-2\"></i>Sign In\n",
        "                    </button>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </header>\n",
        "\n",
        "    <!-- Main Content -->\n",
        "    <main class=\"flex-grow container mx-auto px-4 py-8\">\n",
        "        <div class=\"max-w-4xl mx-auto\">\n",
        "            <div class=\"text-center mb-12\">\n",
        "                <h2 class=\"text-4xl font-bold text-gray-800 mb-3\">Transform Your Imagination into Reality</h2>\n",
        "                <p class=\"text-xl text-gray-600\">Create stunning images with AI. Describe your vision or upload an image to modify.</p>\n",
        "            </div>\n",
        "\n",
        "            <!-- Display Flask Error Messages -->\n",
        "            {% if error %}\n",
        "                <div class=\"flask-error\">\n",
        "                    <strong>Error:</strong> {{ error }}\n",
        "                    {% if error_details %}\n",
        "                        <pre>{{ error_details }}</pre>\n",
        "                    {% endif %}\n",
        "                </div>\n",
        "            {% endif %}\n",
        "\n",
        "            <!-- Generation Form -->\n",
        "            <!-- Ensure form submits via POST and handles file uploads -->\n",
        "            <form id=\"generate-form\" method=\"post\" enctype=\"multipart/form-data\" action=\"{{ url_for('index') }}\">\n",
        "                <div class=\"bg-white rounded-xl shadow-lg p-6 mb-8\">\n",
        "                    <div class=\"mb-6\">\n",
        "                        <label for=\"prompt\" class=\"block text-lg font-medium text-gray-700 mb-2\">\n",
        "                            <i class=\"fas fa-magic mr-2 text-indigo-500\"></i>Describe your dream image\n",
        "                        </label>\n",
        "                        <textarea\n",
        "                            id=\"prompt\"\n",
        "                            name=\"prompt\"  {# <<< Ensure name attribute is set #}\n",
        "                            rows=\"4\"\n",
        "                            class=\"w-full px-4 py-3 border border-gray-300 rounded-lg focus:ring-2 focus:ring-indigo-500 focus:border-indigo-500 transition\"\n",
        "                            placeholder=\"A majestic lion standing on a cliff at sunset, hyper-realistic, 8K resolution...\"\n",
        "                            required {# <<< Add required attribute for basic HTML validation #}\n",
        "                        >{{ prompt or '' }}</textarea> {# <<< Repopulate prompt #}\n",
        "                    </div>\n",
        "\n",
        "                    <!-- Image Upload -->\n",
        "                    <div class=\"mb-6\">\n",
        "                        <label class=\"block text-lg font-medium text-gray-700 mb-2\">\n",
        "                            <i class=\"fas fa-image mr-2 text-indigo-500\"></i>Upload an image (optional)\n",
        "                        </label>\n",
        "                        <div\n",
        "                            id=\"uploadArea\"\n",
        "                            class=\"upload-area relative rounded-lg p-8 text-center cursor-pointer\"\n",
        "                        >\n",
        "                            <!-- Ensure name attribute is set -->\n",
        "                            <input type=\"file\" id=\"imageUpload\" name=\"input_image\" class=\"hidden\" accept=\"image/*\">\n",
        "                            <div id=\"uploadContent\" class=\"space-y-2\">\n",
        "                                <i class=\"fas fa-cloud-upload-alt text-4xl text-indigo-400\"></i>\n",
        "                                <p class=\"text-gray-600\">Drag & drop your image here or click to browse</p>\n",
        "                                <p class=\"text-sm text-gray-500\">Supports JPG, PNG, WEBP (Max 5MB recommended)</p>\n",
        "                            </div>\n",
        "                            <div id=\"imagePreviewContainer\" class=\"hidden\">\n",
        "                                <div class=\"relative inline-block\">\n",
        "                                    <img id=\"imagePreview\" src=\"#\" alt=\"Preview\" class=\"image-preview max-h-64 rounded-lg\">\n",
        "                                    <button type=\"button\" id=\"removeImage\" class=\"absolute top-2 right-2 bg-red-500 text-white rounded-full p-1 hover:bg-red-600 transition\">\n",
        "                                        <i class=\"fas fa-times\"></i>\n",
        "                                    </button>\n",
        "                                </div>\n",
        "                            </div>\n",
        "                        </div>\n",
        "                    </div>\n",
        "\n",
        "                    <!-- Generate Button -->\n",
        "                    <div class=\"text-center\">\n",
        "                        <button\n",
        "                            id=\"generateBtn\"\n",
        "                            type=\"submit\" {# <<< Ensure type is submit #}\n",
        "                            class=\"px-8 py-3 bg-gradient-to-r from-purple-600 to-indigo-700 text-white text-lg font-semibold rounded-lg hover:opacity-90 transition transform hover:scale-105 shadow-lg disabled:opacity-50 disabled:cursor-not-allowed\"\n",
        "                        >\n",
        "                            <i class=\"fas fa-sparkles mr-2\"></i>Generate Image\n",
        "                        </button>\n",
        "                         <!-- Loading Spinner (hidden initially) -->\n",
        "                         <div id=\"loadingSpinner\" class=\"hidden text-center mt-4\">\n",
        "                             <i class=\"fas fa-spinner fa-spin text-3xl text-indigo-600\"></i>\n",
        "                             <p class=\"text-gray-600 mt-2\">Generating... please wait (this can take a minute or two).</p>\n",
        "                         </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </form>\n",
        "\n",
        "            <!-- Generated Image Display - Controlled by Flask Template Logic -->\n",
        "            {% if image_url %}\n",
        "            <div id=\"resultSection\" class=\"bg-white rounded-xl shadow-lg p-6\">\n",
        "                <h3 class=\"text-2xl font-semibold text-gray-800 mb-4 text-center\">\n",
        "                    <i class=\"fas fa-image mr-2 text-indigo-500\"></i>Your Generated Image\n",
        "                </h3>\n",
        "                 <div class=\"text-center text-gray-600 mb-4\">\n",
        "                     <p><strong>Prompt:</strong> {{ prompt }}</p>\n",
        "                     {% if input_image_filename %}\n",
        "                      <p><strong>Based on input:</strong> {{ input_image_filename }}</p>\n",
        "                     {% endif %}\n",
        "                 </div>\n",
        "                <div class=\"flex justify-center\">\n",
        "                    <div id=\"generatedImageContainer\" class=\"relative\">\n",
        "                        <!-- Inject image URL from Flask -->\n",
        "                        <img id=\"generatedImage\" src=\"{{ image_url }}?t={{ timestamp }}\" alt=\"Generated Image\" class=\"generated-image max-w-full rounded-lg shadow-md\">\n",
        "                        <div class=\"mt-4 flex justify-center space-x-4\">\n",
        "                            <button id=\"downloadBtn\" type=\"button\" class=\"download-btn px-4 py-2 bg-green-500 text-white rounded-lg hover:bg-green-600 transition\">\n",
        "                                <i class=\"fas fa-download mr-2\"></i>Download\n",
        "                            </button>\n",
        "                            <button id=\"regenerateBtn\" type=\"button\" class=\"regenerate-btn px-4 py-2 bg-indigo-500 text-white rounded-lg hover:bg-indigo-600 transition\">\n",
        "                                <i class=\"fas fa-sync-alt mr-2\"></i>Regenerate\n",
        "                            </button>\n",
        "                        </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            {% endif %} {# End if image_url #}\n",
        "\n",
        "        </div>\n",
        "    </main>\n",
        "\n",
        "    <!-- Footer -->\n",
        "    <footer class=\"bg-gray-800 text-white py-6 mt-auto\"> {# Added mt-auto for sticky footer effect #}\n",
        "        <div class=\"container mx-auto px-4\">\n",
        "            <div class=\"flex flex-col md:flex-row justify-between items-center\">\n",
        "                <div class=\"mb-4 md:mb-0\">\n",
        "                    <h2 class=\"text-2xl font-bold gradient-text\">DreamDoodle</h2>\n",
        "                    <p class=\"text-gray-400\">Powered by AI magic</p>\n",
        "                </div>\n",
        "                <div class=\"flex space-x-6\">\n",
        "                    <a href=\"#\" class=\"hover:text-indigo-300 transition\">Terms</a>\n",
        "                    <a href=\"#\" class=\"hover:text-indigo-300 transition\">Privacy</a>\n",
        "                    <a href=\"#\" class=\"hover:text-indigo-300 transition\">Contact</a>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"mt-6 text-center text-gray-400 text-sm\">\n",
        "                © 2024 DreamDoodle. All rights reserved. {# Updated year #}\n",
        "            </div>\n",
        "        </div>\n",
        "    </footer>\n",
        "\n",
        "    <!-- Toast Notification Container -->\n",
        "    <div id=\"toastContainer\" class=\"toast-container\">\n",
        "         <!-- Toast messages will be added here by JS -->\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        document.addEventListener('DOMContentLoaded', function() {\n",
        "            // DOM Elements\n",
        "            const uploadArea = document.getElementById('uploadArea');\n",
        "            const imageUpload = document.getElementById('imageUpload');\n",
        "            const uploadContent = document.getElementById('uploadContent');\n",
        "            const imagePreviewContainer = document.getElementById('imagePreviewContainer');\n",
        "            const imagePreview = document.getElementById('imagePreview');\n",
        "            const removeImage = document.getElementById('removeImage');\n",
        "            const promptInput = document.getElementById('prompt');\n",
        "            const generateBtn = document.getElementById('generateBtn');\n",
        "            const generateForm = document.getElementById('generate-form'); // Get the form itself\n",
        "            const loadingSpinner = document.getElementById('loadingSpinner');\n",
        "            const toastContainer = document.getElementById('toastContainer');\n",
        "\n",
        "            // Result section elements (might not exist on initial load)\n",
        "            const resultSection = document.getElementById('resultSection');\n",
        "            const downloadBtn = document.getElementById('downloadBtn');\n",
        "            const regenerateBtn = document.getElementById('regenerateBtn');\n",
        "            const generatedImage = document.getElementById('generatedImage'); // Needed for download\n",
        "\n",
        "            let uploadedFile = null; // Store the File object for resubmission\n",
        "\n",
        "            // --- Event Listeners ---\n",
        "\n",
        "            // Handle drag and drop\n",
        "            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n",
        "                uploadArea.addEventListener(eventName, preventDefaults, false);\n",
        "            });\n",
        "\n",
        "            function preventDefaults(e) {\n",
        "                e.preventDefault();\n",
        "                e.stopPropagation();\n",
        "            }\n",
        "\n",
        "            ['dragenter', 'dragover'].forEach(eventName => {\n",
        "                uploadArea.addEventListener(eventName, () => uploadArea.classList.add('dragover'), false);\n",
        "            });\n",
        "\n",
        "            ['dragleave', 'drop'].forEach(eventName => {\n",
        "                uploadArea.addEventListener(eventName, () => uploadArea.classList.remove('dragover'), false);\n",
        "            });\n",
        "\n",
        "            uploadArea.addEventListener('drop', handleDrop, false);\n",
        "            uploadArea.addEventListener('click', () => imageUpload.click()); // Trigger file input click\n",
        "\n",
        "            function handleDrop(e) {\n",
        "                const dt = e.dataTransfer;\n",
        "                handleFiles(dt.files);\n",
        "            }\n",
        "\n",
        "            // Handle file selection via click\n",
        "            imageUpload.addEventListener('change', function() {\n",
        "                handleFiles(this.files);\n",
        "            });\n",
        "\n",
        "            function handleFiles(files) {\n",
        "                if (files.length > 0) {\n",
        "                    const file = files[0];\n",
        "                    if (file.type.startsWith('image/')) {\n",
        "                        // Simple size check (adjust limit if needed)\n",
        "                        if (file.size > 10 * 1024 * 1024) { // 10 MB limit example\n",
        "                            showToast('Image size should be less than 10MB');\n",
        "                            resetUpload();\n",
        "                            return;\n",
        "                        }\n",
        "\n",
        "                        uploadedFile = file; // Store the actual file object\n",
        "\n",
        "                        const reader = new FileReader();\n",
        "                        reader.onload = function(e) {\n",
        "                            imagePreview.src = e.target.result; // Use reader result for preview\n",
        "                            uploadContent.classList.add('hidden');\n",
        "                            imagePreviewContainer.classList.remove('hidden');\n",
        "                        }\n",
        "                        reader.readAsDataURL(file);\n",
        "                    } else {\n",
        "                        showToast('Please upload a valid image file (JPG, PNG, WEBP).');\n",
        "                        resetUpload();\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "             // Remove uploaded image preview\n",
        "             removeImage.addEventListener('click', function(e) {\n",
        "                 e.stopPropagation(); // Prevent triggering upload area click\n",
        "                 resetUpload();\n",
        "             });\n",
        "\n",
        "             function resetUpload() {\n",
        "                 uploadedFile = null;\n",
        "                 imagePreview.src = '#';\n",
        "                 imageUpload.value = ''; // Clear the file input\n",
        "                 uploadContent.classList.remove('hidden');\n",
        "                 imagePreviewContainer.classList.add('hidden');\n",
        "             }\n",
        "\n",
        "            // Handle Form Submission\n",
        "            generateForm.addEventListener('submit', function(event) {\n",
        "                const promptValue = promptInput.value.trim();\n",
        "\n",
        "                // Client-side validation\n",
        "                if (!promptValue) {\n",
        "                    showToast('Please enter a prompt to generate an image.');\n",
        "                    event.preventDefault(); // Stop submission\n",
        "                    return;\n",
        "                }\n",
        "                // The requirement \"cannot submit with only image\" is handled by requiring the prompt.\n",
        "\n",
        "                // Show loading state AFTER validation passes\n",
        "                generateBtn.disabled = true;\n",
        "                generateBtn.innerHTML = '<i class=\"fas fa-spinner fa-spin mr-2\"></i>Generating...';\n",
        "                loadingSpinner.classList.remove('hidden');\n",
        "\n",
        "                // Allow the form to submit naturally to the Flask backend\n",
        "            });\n",
        "\n",
        "\n",
        "            // Add functionality to Download button (if it exists on the page)\n",
        "            if (downloadBtn && generatedImage) {\n",
        "                 downloadBtn.addEventListener('click', function() {\n",
        "                     const imageUrl = generatedImage.src.split('?')[0]; // Get URL without timestamp\n",
        "                     const filename = imageUrl.substring(imageUrl.lastIndexOf('/') + 1) || 'dreamdoodle_image.png';\n",
        "\n",
        "                     // Create temporary link to trigger download\n",
        "                     const link = document.createElement('a');\n",
        "                     link.href = imageUrl;\n",
        "                     link.download = filename;\n",
        "                     document.body.appendChild(link);\n",
        "                     link.click();\n",
        "                     document.body.removeChild(link);\n",
        "                 });\n",
        "            }\n",
        "\n",
        "             // Add functionality to Regenerate button (if it exists)\n",
        "             if (regenerateBtn) {\n",
        "                 regenerateBtn.addEventListener('click', function() {\n",
        "                     // Re-enable button temporarily if needed, show spinner, and resubmit\n",
        "                     // Note: Resubmitting the form might not re-upload the same file easily across all browsers.\n",
        "                     // A more robust way might involve storing form data and resubmitting via JS fetch,\n",
        "                     // but for simplicity, we'll just trigger the button click again, assuming the user\n",
        "                     // hasn't changed the prompt/image significantly.\n",
        "                     console.log(\"Regenerate clicked\");\n",
        "                     // Ensure prompt is still populated from the template rendering\n",
        "                     if (!promptInput.value.trim()){\n",
        "                         showToast(\"Cannot regenerate without a prompt.\");\n",
        "                         return;\n",
        "                     }\n",
        "\n",
        "                      // Simulate clicking the main generate button again\n",
        "                      generateBtn.disabled = false; // Re-enable briefly if needed\n",
        "                      generateBtn.click(); // This will trigger the form submit listener again\n",
        "                 });\n",
        "             }\n",
        "\n",
        "\n",
        "            // Show toast message function\n",
        "            function showToast(message, type = 'error') {\n",
        "                 const toast = document.createElement('div');\n",
        "                 toast.className = 'toast mb-2'; // Add margin between toasts if multiple show quickly\n",
        "                 let iconClass = 'fa-exclamation-circle';\n",
        "                 let bgColor = 'bg-red-500'; // Default error\n",
        "\n",
        "                 // Add more types if needed (e.g., success, info)\n",
        "                 // if (type === 'success') { iconClass = 'fa-check-circle'; bgColor = 'bg-green-500'; }\n",
        "                 // if (type === 'info') { iconClass = 'fa-info-circle'; bgColor = 'bg-blue-500'; }\n",
        "\n",
        "                 toast.classList.add(bgColor); // Apply background based on type\n",
        "\n",
        "                 toast.innerHTML = `\n",
        "                     <i class=\"fas ${iconClass} mr-2\"></i>\n",
        "                     <span>${message}</span>\n",
        "                 `;\n",
        "                 toastContainer.appendChild(toast);\n",
        "\n",
        "                 // Automatically remove the toast after ~3 seconds\n",
        "                 setTimeout(() => {\n",
        "                     toast.remove();\n",
        "                 }, 3000);\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pybeY5oZRBdY",
        "outputId": "929c8707-ba45-4f6c-a499-b994530ed09e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing templates/index.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import uuid # For unique filenames\n",
        "import time\n",
        "from flask import Flask, request, render_template, redirect, url_for, flash, send_from_directory\n",
        "from werkzeug.utils import secure_filename # For safe filename handling\n",
        "\n",
        "# --- Configuration ---\n",
        "# Use absolute paths in Colab environment\n",
        "APP_ROOT = '/content/DreamDoodleWebApp' # Base directory\n",
        "UPLOAD_FOLDER = os.path.join(APP_ROOT, 'static', 'uploads')\n",
        "RESULT_FOLDER = os.path.join(APP_ROOT, 'static', 'results')\n",
        "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'webp'} # Allowed image upload types\n",
        "\n",
        "# Ensure upload and result directories exist\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "os.makedirs(RESULT_FOLDER, exist_ok=True)\n",
        "\n",
        "app = Flask(__name__, static_folder=os.path.join(APP_ROOT, 'static')) # Point static folder correctly\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "app.config['RESULT_FOLDER'] = RESULT_FOLDER\n",
        "app.secret_key = b'_5#y2L\"F4Q8z\\n\\xec]/' # Needed for flashing messages (change in production)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def allowed_file(filename):\n",
        "    \"\"\"Checks if the uploaded file extension is allowed.\"\"\"\n",
        "    return '.' in filename and \\\n",
        "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "def run_generation(prompt, input_image_path=None):\n",
        "    \"\"\"\n",
        "    Runs the generate.py script with the given arguments.\n",
        "    Returns (output_image_url_path, error_message, error_details)\n",
        "    Note: Returns URL *path* relative to static folder, not full URL\n",
        "    \"\"\"\n",
        "    unique_id = str(uuid.uuid4())\n",
        "    output_filename = f\"{unique_id}.png\" # Assume PNG output\n",
        "    # Use absolute path for the subprocess command\n",
        "    output_image_path_abs = os.path.join(app.config['RESULT_FOLDER'], output_filename)\n",
        "    # This is the path relative to static folder needed for url_for\n",
        "    output_image_url_path = os.path.join('results', output_filename)\n",
        "\n",
        "    # --- Build the command for generate.py ---\n",
        "    cmd = [\n",
        "        'python3',\n",
        "        os.path.join(APP_ROOT, 'generate.py'), # Use absolute path to generate.py\n",
        "        '--prompt', prompt,\n",
        "        '--output-image', output_image_path_abs, # Pass absolute path to script\n",
        "        # Add other default arguments you want here:\n",
        "        '--compiler', 'nexfort', # Keep nexfort for GPU usage\n",
        "        '--steps', '25',         # Adjust default steps\n",
        "        '--seed', str(int(time.time())), # Use time as a somewhat random seed\n",
        "        # '--height', '512', # Optional: set defaults if needed\n",
        "        # '--width', '512',\n",
        "    ]\n",
        "\n",
        "    # Add input image argument if provided (absolute path)\n",
        "    if input_image_path:\n",
        "        cmd.extend(['--input-image', input_image_path])\n",
        "\n",
        "    print(f\"Running command: {' '.join(cmd)}\")\n",
        "\n",
        "    try:\n",
        "        # Execute the script\n",
        "        process = subprocess.run(\n",
        "            cmd,\n",
        "            check=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=600 # Increase timeout for Colab (e.g., 10 minutes)\n",
        "        )\n",
        "        print(\"Generation script stdout:\")\n",
        "        print(process.stdout)\n",
        "        print(\"Generation script stderr:\")\n",
        "        print(process.stderr)\n",
        "\n",
        "        # Return the URL path relative to static\n",
        "        return output_image_url_path, None, None\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Generation script failed with exit code {e.returncode}\")\n",
        "        print(\"Stderr:\")\n",
        "        print(e.stderr)\n",
        "        print(\"Stdout:\")\n",
        "        print(e.stdout)\n",
        "        error_message = \"Image generation failed.\"\n",
        "        error_details = e.stderr or e.stdout or \"No output captured.\"\n",
        "        if os.path.exists(output_image_path_abs):\n",
        "            try: os.remove(output_image_path_abs)\n",
        "            except OSError: pass\n",
        "        return None, error_message, error_details\n",
        "\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        print(\"Generation script timed out.\")\n",
        "        print(\"Stderr before timeout:\")\n",
        "        print(e.stderr)\n",
        "        print(\"Stdout before timeout:\")\n",
        "        print(e.stdout)\n",
        "        error_message = f\"Image generation timed out after {e.timeout} seconds.\"\n",
        "        return None, error_message, \"The process took too long to complete.\"\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: generate.py or python3 not found.\")\n",
        "        error_message = \"Server configuration error: Generation script not found.\"\n",
        "        return None, error_message, \"Please ensure generate.py is in the correct location.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        error_message = \"An unexpected server error occurred during generation.\"\n",
        "        return None, error_message, str(e)\n",
        "\n",
        "\n",
        "# --- Flask Routes ---\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    if request.method == 'POST':\n",
        "        prompt = request.form.get('prompt', '').strip()\n",
        "        image_file = request.files.get('input_image')\n",
        "\n",
        "        if not prompt:\n",
        "            flash(\"Prompt is required.\", \"error\")\n",
        "            return render_template('index.html', error=\"Prompt is required.\")\n",
        "\n",
        "        input_image_path_abs = None\n",
        "        input_image_filename_orig = None\n",
        "\n",
        "        if image_file and image_file.filename != '':\n",
        "            if allowed_file(image_file.filename):\n",
        "                original_filename = secure_filename(image_file.filename)\n",
        "                unique_suffix = str(uuid.uuid4())[:8]\n",
        "                input_filename = f\"{unique_suffix}_{original_filename}\"\n",
        "                # Save to absolute path\n",
        "                input_image_path_abs = os.path.join(app.config['UPLOAD_FOLDER'], input_filename)\n",
        "                try:\n",
        "                    image_file.save(input_image_path_abs)\n",
        "                    input_image_filename_orig = original_filename\n",
        "                    print(f\"Input image saved to: {input_image_path_abs}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving uploaded image: {e}\")\n",
        "                    flash(f\"Error saving uploaded image: {e}\", \"error\")\n",
        "                    return render_template('index.html', error=f\"Could not save uploaded image.\", prompt=prompt)\n",
        "            else:\n",
        "                flash(\"Invalid image file type. Allowed types: png, jpg, jpeg, webp\", \"error\")\n",
        "                return render_template('index.html', error=\"Invalid image file type.\", prompt=prompt)\n",
        "\n",
        "        print(\"Starting generation process...\")\n",
        "        output_image_url_path, error_message, error_details = run_generation(prompt, input_image_path_abs)\n",
        "        print(\"Generation process finished.\")\n",
        "\n",
        "        final_image_url = None\n",
        "        if output_image_url_path:\n",
        "            # Generate the full URL using url_for AFTER generation succeeds\n",
        "            final_image_url = url_for('static', filename=output_image_url_path)\n",
        "\n",
        "        timestamp = int(time.time())\n",
        "        return render_template('index.html',\n",
        "                               image_url=final_image_url,\n",
        "                               error=error_message,\n",
        "                               error_details=error_details,\n",
        "                               prompt=prompt,\n",
        "                               input_image_filename=input_image_filename_orig,\n",
        "                               timestamp=timestamp,\n",
        "                               loading=False\n",
        "                               )\n",
        "\n",
        "    # GET Request\n",
        "    return render_template('index.html', prompt=None, loading=False)\n",
        "\n",
        "# --- Add static file serving route if needed (Flask usually handles this with url_for) ---\n",
        "# Though url_for should work with the static_folder config, sometimes explicitly adding helps\n",
        "# @app.route('/static/<path:filename>')\n",
        "# def static_files(filename):\n",
        "#    return send_from_directory(app.static_folder, filename)\n",
        "\n",
        "\n",
        "# --- Run the App (Specific setup for Colab with ngrok) ---\n",
        "# This part will be executed in a separate cell later\n",
        "def run_app():\n",
        "     # Use host='0.0.0.0' to listen on all interfaces within the container\n",
        "     # Use a specific port\n",
        "     app.run(host='0.0.0.0', port=5001) # Don't use debug=True in the final Colab run usually\n",
        "\n",
        "# We define run_app but don't call it here. We'll call it in the execution cell."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8iXMpyURMZ7",
        "outputId": "84ea8c98-0a58-4521-c0c8-97718b45c973"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Flask and ngrok wrapper\n",
        "!pip install -q Flask pyngrok\n",
        "\n",
        "# Install ML dependencies (copy from your previous successful install)\n",
        "!python3 -m pip install -q -U torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
        "!python3 -m pip install -q -U diffusers transformers accelerate Pillow matplotlib pandas numpy\n",
        "!python3 -m pip install -q -U nexfort torchao # Install latest torchao\n",
        "!python3 -m pip install -q --pre onediff onediffx\n",
        "# !python3 -m pip install -q oneflow # Optional"
      ],
      "metadata": {
        "id": "SzR8PWNyRW6W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import os\n",
        "\n",
        "# --- ngrok Configuration ---\n",
        "# !!! PASTE YOUR NGROK AUTHTOKEN BELOW !!!\n",
        "NGROK_AUTH_TOKEN = \"2waSJAycUk9SskHq1nASfIHCw6A_7gpnRAksSww8dN7BAbRCt\" # <--- PASTE YOUR TOKEN HERE\n",
        "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "\n",
        "# Set region if needed (e.g., 'us', 'eu', 'ap', 'au', 'sa', 'jp', 'in')\n",
        "# conf.get_default().region = 'us' # Usually not needed, but uncomment if you have issues\n",
        "\n",
        "# Import the app object from your app.py file\n",
        "from app import app\n",
        "\n",
        "# Terminate any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# --- Start ngrok tunnel ---\n",
        "try:\n",
        "    # Use port 5001 (or whichever port you specified in app.run)\n",
        "    PORT = 5001\n",
        "    public_url = ngrok.connect(PORT, \"http\").public_url\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"✅ DreamDoodle is running! Access it publicly at: {public_url}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"NOTE: The first image generation might take several minutes due to model loading/compilation.\")\n",
        "    print(\"Keep this Colab cell running to keep the app alive.\")\n",
        "    print(\"Close the tunnel by stopping this cell (Ctrl+C doesn't always work reliably).\")\n",
        "\n",
        "    # --- Run the Flask app ---\n",
        "    app.run(host='0.0.0.0', port=PORT) # Don't use debug=True here normally\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to start ngrok or Flask app: {e}\")\n",
        "    print(\"Check your ngrok setup (authtoken?) and Flask app code.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5TFCOm3aW3_",
        "outputId": "1df084b1-922a-48d9-e4c1-e1b5159fd5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "✅ DreamDoodle is running! Access it publicly at: https://5488-34-83-17-56.ngrok-free.app\n",
            "================================================================================\n",
            "NOTE: The first image generation might take several minutes due to model loading/compilation.\n",
            "Keep this Colab cell running to keep the app alive.\n",
            "Close the tunnel by stopping this cell (Ctrl+C doesn't always work reliably).\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5001\n",
            " * Running on http://172.28.0.12:5001\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:43:29] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:43:30] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:43:31] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:45:51] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:45:52] \"GET / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image saved to: /content/DreamDoodleWebApp/static/uploads/553bfe2f_Screenshot_2025-05-04_151641.png\n",
            "Starting generation process...\n",
            "Running command: python3 /content/DreamDoodleWebApp/generate.py --prompt add a man standing besides him --output-image /content/DreamDoodleWebApp/static/results/2660099d-b7a5-4ff6-9eb3-455185b730e9.png --compiler nexfort --steps 25 --seed 1746352178 --input-image /content/DreamDoodleWebApp/static/uploads/553bfe2f_Screenshot_2025-05-04_151641.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:53:01] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation script stdout:\n",
            "Detected Image-to-Image task (input image provided).\n",
            "Using AutoPipelineForImage2Image.\n",
            "\n",
            "Loading model: SG161222/RealVisXL_V4.0\n",
            "Using scheduler: EulerAncestralDiscreteScheduler\n",
            "Pipeline moved to device: cuda\n",
            "Auto-detected resolution: 1024x1024\n",
            "Adjusted resolution to be multiples of 8: 1024x1024\n",
            "\n",
            "Applying compiler: nexfort\n",
            "Using default compiler config: {'mode': 'max-optimize:max-autotune:freezing', 'memory_format': 'channels_last'}\n",
            "Error during Nexfort setup: The torch version(torch==2.4.0+cu121) of nexfort's compilation environment conflicts with the current environment(torch==2.3.0+cu121)!\n",
            "You can handle this exception in one of two ways:\n",
            "1. Reinstall nextort using one of the following commands:\n",
            "   a. For CN users\n",
            "      python3 -m pip uninstall nexfort -y && python3 -m pip --no-cache-dir install --pre nexfort -f https://nexfort-whl.oss-cn-beijing.aliyuncs.com/torch2.3.0/cu121/\n",
            "   b. For NA/EU users\n",
            "      python3 -m pip uninstall nexfort -y && python3 -m pip --no-cache-dir install --pre nexfort -f https://github.com/siliconflow/nexfort_releases/releases/expanded_assets/torch2.3.0_cu121\n",
            "2. Install torch with version 2.4.0+cu121. Proceeding without nexfort.\n",
            "Loading input image: /content/DreamDoodleWebApp/static/uploads/553bfe2f_Screenshot_2025-05-04_151641.png\n",
            "Input image resized to 1024x1024\n",
            "\n",
            "=======================================\n",
            "Begin warmup (1 runs)...\n",
            "End warmup\n",
            "Warmup time: 10.248s\n",
            "=======================================\n",
            "\n",
            "=======================================\n",
            "Begin timed inference run...\n",
            "Iteration profiler attached via callback_on_step_end.\n",
            "Inference complete.\n",
            "=======================================\n",
            "Task Type: IMAGE2IMAGE\n",
            "Model: SG161222/RealVisXL_V4.0\n",
            "Resolution: 1024x1024\n",
            "Steps: 25\n",
            "Inference Time (Wall Clock): 8.849s\n",
            "Iterations Per Second (GPU Profiled): 1.264\n",
            "Max CUDA Memory Used: 10.984 GiB\n",
            "=======================================\n",
            "Output image saved to: /content/DreamDoodleWebApp/static/results/2660099d-b7a5-4ff6-9eb3-455185b730e9.png\n",
            "\n",
            "Generation script stderr:\n",
            "2025-05-04 09:49:49.681688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746352189.927297    6759 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746352189.996710    6759 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 09:49:50.514438: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]\n",
            "Fetching 18 files:  11%|█         | 2/18 [00:00<00:01, 10.44it/s]\n",
            "Fetching 18 files:  22%|██▏       | 4/18 [00:07<00:32,  2.33s/it]\n",
            "Fetching 18 files:  33%|███▎      | 6/18 [00:39<01:42,  8.56s/it]\n",
            "Fetching 18 files:  89%|████████▉ | 16/18 [01:27<00:11,  5.68s/it]\n",
            "Fetching 18 files: 100%|██████████| 18/18 [01:27<00:00,  4.88s/it]\n",
            "\n",
            "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\n",
            "Loading pipeline components...:  14%|█▍        | 1/7 [00:49<04:57, 49.57s/it]\n",
            "Loading pipeline components...:  43%|████▎     | 3/7 [00:49<00:51, 12.92s/it]\n",
            "Loading pipeline components...:  43%|████▎     | 3/7 [01:00<00:51, 12.92s/it]\n",
            "Loading pipeline components...:  71%|███████▏  | 5/7 [01:05<00:20, 10.31s/it]\n",
            "Loading pipeline components...:  86%|████████▌ | 6/7 [01:05<00:07,  7.58s/it]\n",
            "Loading pipeline components...: 100%|██████████| 7/7 [01:07<00:00,  5.96s/it]\n",
            "Loading pipeline components...: 100%|██████████| 7/7 [01:07<00:00,  9.62s/it]\n",
            "\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]\n",
            " 14%|█▍        | 1/7 [00:00<00:05,  1.07it/s]\n",
            " 29%|██▊       | 2/7 [00:01<00:02,  1.71it/s]\n",
            " 43%|████▎     | 3/7 [00:02<00:02,  1.51it/s]\n",
            " 57%|█████▋    | 4/7 [00:02<00:02,  1.42it/s]\n",
            " 71%|███████▏  | 5/7 [00:03<00:01,  1.37it/s]\n",
            " 86%|████████▌ | 6/7 [00:04<00:00,  1.34it/s]\n",
            "100%|██████████| 7/7 [00:05<00:00,  1.33it/s]\n",
            "100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n",
            "\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]\n",
            " 14%|█▍        | 1/7 [00:00<00:01,  3.16it/s]\n",
            " 29%|██▊       | 2/7 [00:01<00:02,  1.68it/s]\n",
            " 43%|████▎     | 3/7 [00:01<00:02,  1.46it/s]\n",
            " 57%|█████▋    | 4/7 [00:02<00:02,  1.38it/s]\n",
            " 71%|███████▏  | 5/7 [00:03<00:01,  1.34it/s]\n",
            " 86%|████████▌ | 6/7 [00:04<00:00,  1.30it/s]\n",
            "100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n",
            "100%|██████████| 7/7 [00:05<00:00,  1.38it/s]\n",
            "\n",
            "Generation process finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:53:01] \"GET /static/results/2660099d-b7a5-4ff6-9eb3-455185b730e9.png?t=1746352381 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:53:01] \"GET / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting generation process...\n",
            "Running command: python3 /content/DreamDoodleWebApp/generate.py --prompt image of a cat standing on human lap --output-image /content/DreamDoodleWebApp/static/results/01742ac9-3dd0-4b70-81b3-54f5940ca46c.png --compiler nexfort --steps 25 --seed 1746352422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:55:53] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation script stdout:\n",
            "Detected Text-to-Image task (no input image provided).\n",
            "Using AutoPipelineForText2Image.\n",
            "\n",
            "Loading model: SG161222/RealVisXL_V4.0\n",
            "Using scheduler: EulerAncestralDiscreteScheduler\n",
            "Pipeline moved to device: cuda\n",
            "Auto-detected resolution: 1024x1024\n",
            "Adjusted resolution to be multiples of 8: 1024x1024\n",
            "\n",
            "Applying compiler: nexfort\n",
            "Using default compiler config: {'mode': 'max-optimize:max-autotune:freezing', 'memory_format': 'channels_last'}\n",
            "Error during Nexfort setup: The torch version(torch==2.4.0+cu121) of nexfort's compilation environment conflicts with the current environment(torch==2.3.0+cu121)!\n",
            "You can handle this exception in one of two ways:\n",
            "1. Reinstall nextort using one of the following commands:\n",
            "   a. For CN users\n",
            "      python3 -m pip uninstall nexfort -y && python3 -m pip --no-cache-dir install --pre nexfort -f https://nexfort-whl.oss-cn-beijing.aliyuncs.com/torch2.3.0/cu121/\n",
            "   b. For NA/EU users\n",
            "      python3 -m pip uninstall nexfort -y && python3 -m pip --no-cache-dir install --pre nexfort -f https://github.com/siliconflow/nexfort_releases/releases/expanded_assets/torch2.3.0_cu121\n",
            "2. Install torch with version 2.4.0+cu121. Proceeding without nexfort.\n",
            "\n",
            "=======================================\n",
            "Begin warmup (1 runs)...\n",
            "End warmup\n",
            "Warmup time: 23.541s\n",
            "=======================================\n",
            "\n",
            "=======================================\n",
            "Begin timed inference run...\n",
            "Iteration profiler attached via callback_on_step_end.\n",
            "Inference complete.\n",
            "=======================================\n",
            "Task Type: TEXT2IMAGE\n",
            "Model: SG161222/RealVisXL_V4.0\n",
            "Resolution: 1024x1024\n",
            "Steps: 25\n",
            "Inference Time (Wall Clock): 23.096s\n",
            "Iterations Per Second (GPU Profiled): 1.181\n",
            "Max CUDA Memory Used: 10.962 GiB\n",
            "=======================================\n",
            "Output image saved to: /content/DreamDoodleWebApp/static/results/01742ac9-3dd0-4b70-81b3-54f5940ca46c.png\n",
            "\n",
            "Generation script stderr:\n",
            "2025-05-04 09:53:51.215448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746352431.250388    7799 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746352431.261096    7799 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-04 09:53:51.293195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\n",
            "Loading pipeline components...:  14%|█▍        | 1/7 [00:02<00:14,  2.34s/it]\n",
            "Loading pipeline components...:  29%|██▊       | 2/7 [00:02<00:05,  1.12s/it]\n",
            "Loading pipeline components...:  57%|█████▋    | 4/7 [00:02<00:01,  2.20it/s]\n",
            "Loading pipeline components...:  86%|████████▌ | 6/7 [00:50<00:11, 11.59s/it]\n",
            "Loading pipeline components...: 100%|██████████| 7/7 [01:04<00:00, 12.02s/it]\n",
            "Loading pipeline components...: 100%|██████████| 7/7 [01:04<00:00,  9.15s/it]\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\n",
            "  4%|▍         | 1/25 [00:01<00:41,  1.73s/it]\n",
            "  8%|▊         | 2/25 [00:02<00:20,  1.11it/s]\n",
            " 12%|█▏        | 3/25 [00:02<00:18,  1.18it/s]\n",
            " 16%|█▌        | 4/25 [00:03<00:17,  1.21it/s]\n",
            " 20%|██        | 5/25 [00:04<00:16,  1.22it/s]\n",
            " 24%|██▍       | 6/25 [00:05<00:15,  1.24it/s]\n",
            " 28%|██▊       | 7/25 [00:06<00:14,  1.25it/s]\n",
            " 32%|███▏      | 8/25 [00:06<00:13,  1.25it/s]\n",
            " 36%|███▌      | 9/25 [00:07<00:12,  1.25it/s]\n",
            " 40%|████      | 10/25 [00:08<00:11,  1.26it/s]\n",
            " 44%|████▍     | 11/25 [00:09<00:11,  1.26it/s]\n",
            " 48%|████▊     | 12/25 [00:09<00:10,  1.26it/s]\n",
            " 52%|█████▏    | 13/25 [00:10<00:09,  1.25it/s]\n",
            " 56%|█████▌    | 14/25 [00:11<00:08,  1.25it/s]\n",
            " 60%|██████    | 15/25 [00:12<00:07,  1.25it/s]\n",
            " 64%|██████▍   | 16/25 [00:13<00:07,  1.25it/s]\n",
            " 68%|██████▊   | 17/25 [00:13<00:06,  1.25it/s]\n",
            " 72%|███████▏  | 18/25 [00:14<00:05,  1.24it/s]\n",
            " 76%|███████▌  | 19/25 [00:15<00:04,  1.24it/s]\n",
            " 80%|████████  | 20/25 [00:16<00:04,  1.24it/s]\n",
            " 84%|████████▍ | 21/25 [00:17<00:03,  1.24it/s]\n",
            " 88%|████████▊ | 22/25 [00:18<00:02,  1.24it/s]\n",
            " 92%|█████████▏| 23/25 [00:18<00:01,  1.24it/s]\n",
            " 96%|█████████▌| 24/25 [00:19<00:00,  1.24it/s]\n",
            "100%|██████████| 25/25 [00:20<00:00,  1.24it/s]\n",
            "100%|██████████| 25/25 [00:20<00:00,  1.22it/s]\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\n",
            "  4%|▍         | 1/25 [00:00<00:07,  3.08it/s]\n",
            "  8%|▊         | 2/25 [00:01<00:14,  1.61it/s]\n",
            " 12%|█▏        | 3/25 [00:01<00:15,  1.40it/s]\n",
            " 16%|█▌        | 4/25 [00:02<00:15,  1.33it/s]\n",
            " 20%|██        | 5/25 [00:03<00:15,  1.28it/s]\n",
            " 24%|██▍       | 6/25 [00:04<00:15,  1.25it/s]\n",
            " 28%|██▊       | 7/25 [00:05<00:14,  1.24it/s]\n",
            " 32%|███▏      | 8/25 [00:06<00:13,  1.23it/s]\n",
            " 36%|███▌      | 9/25 [00:06<00:13,  1.23it/s]\n",
            " 40%|████      | 10/25 [00:07<00:12,  1.21it/s]\n",
            " 44%|████▍     | 11/25 [00:08<00:11,  1.21it/s]\n",
            " 48%|████▊     | 12/25 [00:09<00:10,  1.20it/s]\n",
            " 52%|█████▏    | 13/25 [00:10<00:10,  1.20it/s]\n",
            " 56%|█████▌    | 14/25 [00:11<00:09,  1.19it/s]\n",
            " 60%|██████    | 15/25 [00:11<00:08,  1.19it/s]\n",
            " 64%|██████▍   | 16/25 [00:12<00:07,  1.18it/s]\n",
            " 68%|██████▊   | 17/25 [00:13<00:06,  1.18it/s]\n",
            " 72%|███████▏  | 18/25 [00:14<00:05,  1.17it/s]\n",
            " 76%|███████▌  | 19/25 [00:15<00:05,  1.17it/s]\n",
            " 80%|████████  | 20/25 [00:16<00:04,  1.16it/s]\n",
            " 84%|████████▍ | 21/25 [00:17<00:03,  1.16it/s]\n",
            " 88%|████████▊ | 22/25 [00:18<00:02,  1.16it/s]\n",
            " 92%|█████████▏| 23/25 [00:18<00:01,  1.16it/s]\n",
            " 96%|█████████▌| 24/25 [00:19<00:00,  1.16it/s]\n",
            "100%|██████████| 25/25 [00:20<00:00,  1.16it/s]\n",
            "100%|██████████| 25/25 [00:20<00:00,  1.21it/s]\n",
            "\n",
            "Generation process finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:55:54] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:55:54] \"GET /static/results/01742ac9-3dd0-4b70-81b3-54f5940ca46c.png?t=1746352553 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [04/May/2025 09:56:40] \"GET /static/results/01742ac9-3dd0-4b70-81b3-54f5940ca46c.png HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting generation process...\n",
            "Running command: python3 /content/DreamDoodleWebApp/generate.py --prompt image of a cat standing on human lap --output-image /content/DreamDoodleWebApp/static/results/00bd05ff-825c-438d-8983-718bf1e270b6.png --compiler nexfort --steps 25 --seed 1746352605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5N-x1G4vadNj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}