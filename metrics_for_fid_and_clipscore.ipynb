{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4Qu5BCiJu3VF1e+aRjUXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukiod/T2I-and-I2I-Report/blob/main/metrics_for_fid_and_clipscore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fid Score"
      ],
      "metadata": {
        "id": "iFd6AUtv8rFR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adlRxsyH59cZ",
        "outputId": "9859f378-accf-4afb-9dc4-9c4d02c28904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FID calculation between two images...\n",
            "Image 1: 1.png\n",
            "Image 2: 2.png\n",
            "\n",
            "FID score between the two images: 3.0817925930023193\n",
            "\n",
            "Note: Lower FID scores indicate more similar images.\n",
            "FID typically measures distribution similarity between sets of images.\n",
            "This adaptation works for individual images but should be interpreted accordingly.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from scipy.linalg import sqrtm\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess an image for feature extraction with PyTorch models\n",
        "    \"\"\"\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "    # Define transforms for InceptionV3 - resize to 299x299 and normalize\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Load the image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Apply preprocessing\n",
        "    img_tensor = preprocess(img)\n",
        "\n",
        "    # Add batch dimension\n",
        "    img_tensor = img_tensor.unsqueeze(0)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "class InceptionFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Class to extract features from InceptionV3 model\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(InceptionFeatureExtractor, self).__init__()\n",
        "        # Load pretrained InceptionV3\n",
        "        self.inception = models.inception_v3(pretrained=True)\n",
        "        # Remove the last fully connected layer\n",
        "        self.inception.fc = nn.Identity()\n",
        "        # Set to evaluation mode\n",
        "        self.inception.eval()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set model to not require gradients\n",
        "        with torch.no_grad():\n",
        "            # Need to handle the InceptionV3's auxiliary outputs for compatibility\n",
        "            if self.inception.training:\n",
        "                output, _ = self.inception(x)\n",
        "            else:\n",
        "                output = self.inception(x)\n",
        "        return output\n",
        "\n",
        "def calculate_fid_for_two_images(image_path1, image_path2):\n",
        "    \"\"\"\n",
        "    Calculate FID score between two individual images using PyTorch\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Set device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize feature extractor\n",
        "        feature_extractor = InceptionFeatureExtractor().to(device)\n",
        "\n",
        "        # Load and preprocess images\n",
        "        img1 = load_and_preprocess_image(image_path1).to(device)\n",
        "        img2 = load_and_preprocess_image(image_path2).to(device)\n",
        "\n",
        "        # Extract features\n",
        "        feature1 = feature_extractor(img1).cpu().numpy()\n",
        "        feature2 = feature_extractor(img2).cpu().numpy()\n",
        "\n",
        "        # Calculate mean of features\n",
        "        mean1, mean2 = feature1[0], feature2[0]\n",
        "\n",
        "        # For single images, use a small identity matrix with noise as covariance\n",
        "        feature_dim = feature1.shape[1]\n",
        "        eps = 1e-6\n",
        "        cov1 = np.eye(feature_dim) * eps\n",
        "        cov2 = np.eye(feature_dim) * eps\n",
        "\n",
        "        # Calculate sum of squared difference between means\n",
        "        ssdiff = np.sum((mean1 - mean2) ** 2.0)\n",
        "\n",
        "        # Calculate sqrt of product of covariances\n",
        "        covmean = sqrtm(cov1.dot(cov2))\n",
        "\n",
        "        # Check for complex numbers from sqrtm\n",
        "        if np.iscomplexobj(covmean):\n",
        "            covmean = covmean.real\n",
        "\n",
        "        # Calculate FID\n",
        "        fid = ssdiff + np.trace(cov1 + cov2 - 2.0 * covmean)\n",
        "\n",
        "        return fid\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating FID: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to calculate FID between two 512x512 RGB images\n",
        "    \"\"\"\n",
        "    # Image paths\n",
        "    image_path1 = \"1.png\"  # 512x512 RGB image\n",
        "    image_path2 = \"2.png\"  # 512x512 RGB image\n",
        "\n",
        "    print(\"Starting FID calculation between two images...\")\n",
        "    print(f\"Image 1: {image_path1}\")\n",
        "    print(f\"Image 2: {image_path2}\")\n",
        "\n",
        "    # Calculate FID\n",
        "    fid_score = calculate_fid_for_two_images(image_path1, image_path2)\n",
        "\n",
        "    # Print result\n",
        "    print(f\"\\nFID score between the two images: {fid_score}\")\n",
        "\n",
        "    # Additional information\n",
        "    print(\"\\nNote: Lower FID scores indicate more similar images.\")\n",
        "    print(\"FID typically measures distribution similarity between sets of images.\")\n",
        "    print(\"This adaptation works for individual images but should be interpreted accordingly.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clip Score"
      ],
      "metadata": {
        "id": "Sz8kCzYO8wEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch Pillow requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRjmoH_48nHA",
        "outputId": "ba7e3cc7-a99a-49fa-be7f-41197c630f25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests # Keep for the function's URL capability\n",
        "from io import BytesIO # Keep for the function's URL capability\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import os # To check if path exists\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\" # Or \"openai/clip-vit-large-patch14\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- Load Model and Processor ---\n",
        "# Load the CLIP model and processor just once\n",
        "try:\n",
        "    model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "    processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "    print(f\"Successfully loaded model '{MODEL_NAME}' and processor.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or processor: {e}\")\n",
        "    print(\"Please ensure you have installed the required libraries and have internet access.\")\n",
        "    exit() # Exit if model loading fails\n",
        "\n",
        "# --- Function to Calculate CLIP Score (same as before) ---\n",
        "def get_clip_score(image_path_or_url, text_description):\n",
        "    \"\"\"\n",
        "    Calculates the CLIP score (cosine similarity) between an image and a text description.\n",
        "\n",
        "    Args:\n",
        "        image_path_or_url (str): Path to the local image file OR URL of the image.\n",
        "        text_description (str): The text description to compare with the image.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity score between the image and text embeddings (between -1 and 1).\n",
        "               Returns None if image loading or processing fails.\n",
        "    \"\"\"\n",
        "    image = None\n",
        "    try:\n",
        "        # Check if it's a URL\n",
        "        if image_path_or_url.startswith(\"http://\") or image_path_or_url.startswith(\"https://\"):\n",
        "            response = requests.get(image_path_or_url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "            # print(f\"Successfully loaded image from URL: {image_path_or_url}\") # Less verbose for batch\n",
        "        # Check if it's a local file path\n",
        "        elif os.path.exists(image_path_or_url):\n",
        "            image = Image.open(image_path_or_url).convert(\"RGB\")\n",
        "            # print(f\"Successfully loaded image from path: {image_path_or_url}\") # Less verbose for batch\n",
        "        else:\n",
        "             print(f\"Error: Image path or URL does not exist: {image_path_or_url}\")\n",
        "             return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading image from URL {image_path_or_url}: {e}\")\n",
        "        return None\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Image file not found at: {image_path_or_url}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening or processing image {image_path_or_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "    if image is None:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # --- Preprocess Data ---\n",
        "        inputs = processor(\n",
        "            text=[text_description], # Note: text is always a list\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # --- Get Embeddings ---\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            image_embeds = outputs.image_embeds\n",
        "            text_embeds = outputs.text_embeds\n",
        "\n",
        "        # --- Calculate Cosine Similarity ---\n",
        "        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        cosine_similarity = (image_embeds @ text_embeds.T).item()\n",
        "\n",
        "        return cosine_similarity\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during CLIP processing for {image_path_or_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    image_files = [\"1.png\", \"2.png\"] # Your image filenames\n",
        "    text_prompt = \"turn it into a cyborg\" # Your specific prompt\n",
        "\n",
        "    print(f\"\\nCalculating CLIP scores for prompt: '{text_prompt}'\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    results = {} # Dictionary to store results\n",
        "\n",
        "    for image_path in image_files:\n",
        "        print(f\"Processing: {image_path}\")\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"  WARNING: Image file not found at '{image_path}'. Skipping.\")\n",
        "            results[image_path] = None # Record that it was skipped\n",
        "            continue # Move to the next image\n",
        "\n",
        "        # Calculate the score\n",
        "        score = get_clip_score(image_path, text_prompt)\n",
        "        results[image_path] = score # Store the score (or None if an error occurred)\n",
        "\n",
        "        if score is not None:\n",
        "            print(f\"  CLIP Score: {score:.4f}\")\n",
        "        else:\n",
        "            print(f\"  Failed to calculate CLIP score.\")\n",
        "        print(\"-\" * 10) # Separator for clarity\n",
        "\n",
        "    # --- Summary ---\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(f\"Prompt: '{text_prompt}'\")\n",
        "    for img, score in results.items():\n",
        "        if score is not None:\n",
        "            print(f\"Image: {img} -> Score: {score:.4f}\")\n",
        "        else:\n",
        "            print(f\"Image: {img} -> Score: Failed/Skipped\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyD0_KlX8k3Y",
        "outputId": "e110f444-75a2-4a93-d193-3043205c5f07"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Successfully loaded model 'openai/clip-vit-base-patch32' and processor.\n",
            "\n",
            "Calculating CLIP scores for prompt: 'turn it into a cyborg'\n",
            "------------------------------\n",
            "Processing: 1.png\n",
            "  CLIP Score: 0.2535\n",
            "----------\n",
            "Processing: 2.png\n",
            "  CLIP Score: 0.2515\n",
            "----------\n",
            "\n",
            "--- Summary ---\n",
            "Prompt: 'turn it into a cyborg'\n",
            "Image: 1.png -> Score: 0.2535\n",
            "Image: 2.png -> Score: 0.2515\n"
          ]
        }
      ]
    }
  ]
}